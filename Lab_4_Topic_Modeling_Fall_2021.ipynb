{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4. TOPIC MODELING - ANSWERS\n",
    "\n",
    "### **<font color=green>INSTRUCTIONS:</font>** <br> \n",
    "\n",
    "**<font color=green> 1. Look for EXERCISES in the script (3 in total).</font>** <br>\n",
    "\n",
    "**<font color=green> 2. Each student INDIVIDUALLY uploads this script with their answers embedded to Canvas by the end of the lab session or by Wednesday, 11:59pm CT (St. Louis time).</font>** \n",
    "\n",
    "### Lab Objectives\n",
    "\n",
    "1. Learn how to estimate a topic model in Python (using the sklearn package)\n",
    "2. Get familiar with the output of a topic model\n",
    "3. Visualize topics in a text corpus\n",
    "4. Evaluate and discriminate between topic models\n",
    "\n",
    "### Session Prep\n",
    "Below we install the modules we need and define the text normalization function we used in Lab 3, as well as two addtional function we need for today only.\n",
    "\n",
    "**Important:** Make sure Text_Normalization_Function.ipynb file is in the same directory as the current notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (1.20.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: pyLDAvis in /opt/anaconda3/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.20.1)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: funcy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.24.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.6.2)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Requirement already satisfied: html.parser in /opt/anaconda3/lib/python3.8/site-packages (0.2)\n",
      "Requirement already satisfied: ply in /opt/anaconda3/lib/python3.8/site-packages (from html.parser) (3.11)\n",
      "Requirement already satisfied: pattern3 in /opt/anaconda3/lib/python3.8/site-packages (3.0.0)\n",
      "Requirement already satisfied: pdfminer3k in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (1.3.4)\n",
      "Requirement already satisfied: simplejson in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (3.17.5)\n",
      "Requirement already satisfied: pdfminer.six in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (20211012)\n",
      "Requirement already satisfied: feedparser in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (6.0.8)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (4.9.3)\n",
      "Requirement already satisfied: docx in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: cherrypy in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (18.6.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->pattern3) (2.2.1)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (8.5.2)\n",
      "Requirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (8.7.0)\n",
      "Requirement already satisfied: portend>=2.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: jaraco.collections in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (3.4.0)\n",
      "Requirement already satisfied: zc.lockfile in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: jaraco.functools in /opt/anaconda3/lib/python3.8/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (3.4.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (1.15.0)\n",
      "Requirement already satisfied: tempora>=1.8 in /opt/anaconda3/lib/python3.8/site-packages (from portend>=2.1.1->cherrypy->pattern3) (4.1.2)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.8/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2021.1)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.8/site-packages (from docx->pattern3) (4.6.3)\n",
      "Requirement already satisfied: Pillow>=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from docx->pattern3) (8.2.0)\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.8/site-packages (from feedparser->pattern3) (1.0.0)\n",
      "Requirement already satisfied: jaraco.text in /opt/anaconda3/lib/python3.8/site-packages (from jaraco.collections->cherrypy->pattern3) (3.6.0)\n",
      "Requirement already satisfied: jaraco.classes in /opt/anaconda3/lib/python3.8/site-packages (from jaraco.collections->cherrypy->pattern3) (3.2.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.8/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern3) (3.4.1)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer.six->pattern3) (4.0.0)\n",
      "Requirement already satisfied: cryptography in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer.six->pattern3) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.8/site-packages (from cryptography->pdfminer.six->pattern3) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern3) (2.20)\n",
      "Requirement already satisfied: ply in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from zc.lockfile->cherrypy->pattern3) (52.0.0.post20210125)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /opt/anaconda3/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.20.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.24.1)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.6.2)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: funcy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lilia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
     ]
    }
   ],
   "source": [
    "#the module 'sys' allows istalling module from inside Jupyter\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Natrual Language ToolKit (NLTK)\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "\n",
    "!{sys.executable} -m pip install sklearn\n",
    "from sklearn import metrics\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import  CountVectorizer #bag-of-words vectorizer \n",
    "from sklearn.decomposition import LatentDirichletAllocation #package for LDA\n",
    "\n",
    "# Plotting tools\n",
    "\n",
    "from pprint import pprint\n",
    "!{sys.executable} -m pip install pyLDAvis #visualizing LDA\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#define text normalization function\n",
    "%run ./Text_Normalization_Function.ipynb #defining text normalization function\n",
    "\n",
    "#ignore warnings about future changes in functions as they take too much space\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define two functions that will display the results of fitting a topic model, to be used later:\n",
    "\n",
    "*Note: these functions are not the focus of the lab, therefore we'll not be discussing them, but you are welcome to explore and dig into them later if you prefer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "def get_topic_words(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_words = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_word_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_words.append(keywords.take(top_word_locs).tolist())\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data Example\n",
    "We'll start with working on a toy dataset. It will allow us to grasp the full results of a topic model before moving to high-dimensional realistic data.\n",
    "\n",
    "#### Define and Prep Toy Data\n",
    "Let's use the toy corpus on animals and programming similar to one in the lecture. Let's define it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\"The fox jumps over the dog\", \n",
    "              \"The fox is very clever and quick\", \n",
    "              \"The dog is slow and lazy\", \n",
    "              \"The cat is smarter than the fox and the dog but it can never learn Java\", \n",
    "              \"Python is an excellent programming language\", \n",
    "              \"Java and Ruby are other programming languages\", \n",
    "              \"Python and Java are very popular programming languages\", \n",
    "              \"Python programs are smaller than Java programs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **normalize** our toy_corpus and call the normalized corpus **normalized_toy_corpus**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fox jump dog',\n",
       " 'fox clever quick',\n",
       " 'dog slow lazy',\n",
       " 'cat smarter fox dog never learn java',\n",
       " 'python excellent programming language',\n",
       " 'java ruby programming language',\n",
       " 'python java popular programming language',\n",
       " 'python program small java program']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_toy_corpus = normalize_corpus(toy_corpus) \n",
    "normalized_toy_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for topic modeling we need text data in the **Bag-of-Words** representation, let's **vectorize** our normalized_toy_corpus and call it **bow_toy_corpus**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the bag-of-words vectorizer:\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "#vectorize the normalized data:\n",
    "bow_toy_corpus = bow_vectorizer.fit_transform(normalized_toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_toy_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the Bag-of-Words representation of our corpus: **It never hurts to know how you data look like :)** Note absence of stopwords and other differences with the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>clever</th>\n",
       "      <th>dog</th>\n",
       "      <th>excellent</th>\n",
       "      <th>fox</th>\n",
       "      <th>java</th>\n",
       "      <th>jump</th>\n",
       "      <th>language</th>\n",
       "      <th>lazy</th>\n",
       "      <th>learn</th>\n",
       "      <th>never</th>\n",
       "      <th>popular</th>\n",
       "      <th>program</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>quick</th>\n",
       "      <th>ruby</th>\n",
       "      <th>slow</th>\n",
       "      <th>small</th>\n",
       "      <th>smarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  clever  dog  excellent  fox  java  jump  language  lazy  learn  never  \\\n",
       "0    0       0    1          0    1     0     1         0     0      0      0   \n",
       "1    0       1    0          0    1     0     0         0     0      0      0   \n",
       "2    0       0    1          0    0     0     0         0     1      0      0   \n",
       "3    1       0    1          0    1     1     0         0     0      1      1   \n",
       "4    0       0    0          1    0     0     0         1     0      0      0   \n",
       "5    0       0    0          0    0     1     0         1     0      0      0   \n",
       "6    0       0    0          0    0     1     0         1     0      0      0   \n",
       "7    0       0    0          0    0     1     0         0     0      0      0   \n",
       "\n",
       "   popular  program  programming  python  quick  ruby  slow  small  smarter  \n",
       "0        0        0            0       0      0     0     0      0        0  \n",
       "1        0        0            0       0      1     0     0      0        0  \n",
       "2        0        0            0       0      0     0     1      0        0  \n",
       "3        0        0            0       0      0     0     0      0        1  \n",
       "4        0        0            1       1      0     0     0      0        0  \n",
       "5        0        0            1       0      0     1     0      0        0  \n",
       "6        1        0            1       1      0     0     0      0        0  \n",
       "7        0        2            0       1      0     0     0      1        0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = bow_toy_corpus.todense(), columns = bow_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Model (via Latent Dirichlet Allocation) on Toy Data\n",
    "Now let's **model topics** in our toy data. Given that the toy corpus is so small, we know all \"topics\" it contains (**what are they?**) and it will be easy for us:<br> 1) to check if the topic model results make sense; <br>2) see all the results that the topic model produces.  <br><br>\n",
    "We will be using the **LatentDirichletAllocation** function which we already imported earlier (see Session Prep). The function has the following **parameters** to be set:\n",
    "1. Number of topics to model: **n_components**\n",
    "2. Parameter vector for the Dirichlet distribution for *topics*: **doc_topic_prior**\n",
    "3. Parameter vector for the Dirichlet distribution for *words* in a topic: **topic_word_prior**\n",
    "\n",
    "Notes on **parameter vectors for the Dirichlet distributions**: <br>\n",
    "1. Although the Dirichlet distribution parameters are represented by a **vector**, for simplicity we provide one number for each parameter vector. For example, if we set the number of topics to 2 (n_components=2), the parameter vector for the Dirichlet distribution for *topics* should be a two-dimensional vector. We set doc_topic_prior=0.5 and the LatentDirichletAllocation function internally creates a two-dimensional vector (0.5,0.5). Similar logic applies to the parameter vector for the Dirichlet distribution for *words*.<br><br>\n",
    "2. Remember, that we need **sparsity** in the distribution of topics across documents (i.e., some documents have a zero probability of containing some of the topics) and *sparsity* in the distribution of words in topics (i.e., some words have zero probability to be present in some topics). To induce sparsity, we need to set doc_topic_prior and doc_topic_prior between 0 and 1.\n",
    "\n",
    "Now, let's set the parameters and estimate the topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_toy_corpus = LatentDirichletAllocation(n_components=2, max_iter=500,\n",
    "                                           doc_topic_prior = 0.9,\n",
    "                                           topic_word_prior = 0.9).fit(bow_toy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results by showing 15 **most frequent (top)** words for each topic (we use **function display_topics** defined in Session Prep):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "java python language programming program popular small excellent ruby fox dog clever quick lazy slow\n",
      "Topic 1:\n",
      "dog fox smarter never learn cat jump lazy slow quick clever java programming language python\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 15\n",
    "display_topics(lda_toy_corpus, bow_vectorizer.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that topics do not have names or labels. **Topics are just collections of words**, following the definition of a topic in text mining. <br><br> To be precise, topics are **word vectors**, where each vector element is the **weight** (relative frequency) of the word in a topic. Let's have a look at those \"word vectors\". Can you see below that each word vector (topic) is a **simplex**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>clever</th>\n",
       "      <th>dog</th>\n",
       "      <th>excellent</th>\n",
       "      <th>fox</th>\n",
       "      <th>java</th>\n",
       "      <th>jump</th>\n",
       "      <th>language</th>\n",
       "      <th>lazy</th>\n",
       "      <th>learn</th>\n",
       "      <th>never</th>\n",
       "      <th>popular</th>\n",
       "      <th>program</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>quick</th>\n",
       "      <th>ruby</th>\n",
       "      <th>slow</th>\n",
       "      <th>small</th>\n",
       "      <th>smarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>0.050982</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>0.109888</td>\n",
       "      <td>0.026407</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.051208</td>\n",
       "      <td>0.078480</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.105892</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.050859</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.051163</td>\n",
       "      <td>0.025956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055088</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.113486</td>\n",
       "      <td>0.028202</td>\n",
       "      <td>0.113486</td>\n",
       "      <td>0.053817</td>\n",
       "      <td>0.054604</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.055088</td>\n",
       "      <td>0.055088</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>0.028293</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.028007</td>\n",
       "      <td>0.055088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat    clever       dog  excellent       fox      java      jump  \\\n",
       "0  0.025956  0.026564  0.026766   0.050982  0.026766  0.109888  0.026407   \n",
       "1  0.055088  0.054435  0.113486   0.028202  0.113486  0.053817  0.054604   \n",
       "\n",
       "   language      lazy     learn     never   popular   program  programming  \\\n",
       "0  0.105756  0.026564  0.025956  0.025956  0.051208  0.078480     0.105756   \n",
       "1  0.028622  0.054435  0.055088  0.055088  0.027959  0.028293     0.028622   \n",
       "\n",
       "     python     quick      ruby      slow     small   smarter  \n",
       "0  0.105892  0.026564  0.050859  0.026564  0.051163  0.025956  \n",
       "1  0.028476  0.054435  0.028333  0.054435  0.028007  0.055088  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weights = lda_toy_corpus.components_ / lda_toy_corpus.components_.sum(axis=1)[:, np.newaxis]\n",
    "pd.DataFrame(word_weights.T, index = bow_vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 1:</font>**\n",
    "\n",
    "**<font color=green>1.1. Adjust doc_topic_prior (alpha) and topic_word_prior (beta) and observe the changes in topic representation.</font>** \n",
    "\n",
    "**<font color=green>1.2. You are likely to be not 100% satisfied with the model performance, even after all the adjustments. The word \"java\" might still be appearing among the top words in the \"animals\" topic. Why? Looking at the text corpus might help to find the answer.</font>**\n",
    "\n",
    "**Answer 1.1:** <br>\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "java python language programming program popular small excellent ruby dog fox lazy slow clever quick\n",
      "Topic 1:\n",
      "fox dog smarter never learn cat jump quick clever lazy slow java programming language python\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>clever</th>\n",
       "      <th>dog</th>\n",
       "      <th>excellent</th>\n",
       "      <th>fox</th>\n",
       "      <th>java</th>\n",
       "      <th>jump</th>\n",
       "      <th>language</th>\n",
       "      <th>lazy</th>\n",
       "      <th>learn</th>\n",
       "      <th>never</th>\n",
       "      <th>popular</th>\n",
       "      <th>program</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>quick</th>\n",
       "      <th>ruby</th>\n",
       "      <th>slow</th>\n",
       "      <th>small</th>\n",
       "      <th>smarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.051596</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.113631</td>\n",
       "      <td>0.024541</td>\n",
       "      <td>0.109860</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.051761</td>\n",
       "      <td>0.080823</td>\n",
       "      <td>0.109860</td>\n",
       "      <td>0.109955</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.051511</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.051735</td>\n",
       "      <td>0.024198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055741</td>\n",
       "      <td>0.055275</td>\n",
       "      <td>0.118095</td>\n",
       "      <td>0.026268</td>\n",
       "      <td>0.118095</td>\n",
       "      <td>0.053886</td>\n",
       "      <td>0.055372</td>\n",
       "      <td>0.026492</td>\n",
       "      <td>0.055275</td>\n",
       "      <td>0.055741</td>\n",
       "      <td>0.055741</td>\n",
       "      <td>0.026091</td>\n",
       "      <td>0.026277</td>\n",
       "      <td>0.026492</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.055275</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.055275</td>\n",
       "      <td>0.026118</td>\n",
       "      <td>0.055741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat    clever       dog  excellent       fox      java      jump  \\\n",
       "0  0.024198  0.024631  0.024706   0.051596  0.024706  0.113631  0.024541   \n",
       "1  0.055741  0.055275  0.118095   0.026268  0.118095  0.053886  0.055372   \n",
       "\n",
       "   language      lazy     learn     never   popular   program  programming  \\\n",
       "0  0.109860  0.024631  0.024198  0.024198  0.051761  0.080823     0.109860   \n",
       "1  0.026492  0.055275  0.055741  0.055741  0.026091  0.026277     0.026492   \n",
       "\n",
       "     python     quick      ruby      slow     small   smarter  \n",
       "0  0.109955  0.024631  0.051511  0.024631  0.051735  0.024198  \n",
       "1  0.026390  0.055275  0.026359  0.055275  0.026118  0.055741  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_toy_corpus = LatentDirichletAllocation(n_components=2, max_iter=500,\n",
    "                                           doc_topic_prior = 0.8,\n",
    "                                           topic_word_prior = 0.8).fit(bow_toy_corpus)\n",
    "display_topics(lda_toy_corpus, bow_vectorizer.get_feature_names(), no_top_words)\n",
    "word_weights = lda_toy_corpus.components_ / lda_toy_corpus.components_.sum(axis=1)[:, np.newaxis]\n",
    "pd.DataFrame(word_weights.T, index = bow_vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alpha and beta change, the top words change accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1.2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we only have 8 corpus and one of them is \"The cat is smarter than the fox and the dog but it can never learn Java\", which is the only corpus with both animals and programming, and only the word \"Java\" exists in the corpus while other words in the programming topic does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling on Real Data\n",
    "\n",
    "The dataset here is the one we used for doing Text Classification in Lab 3. The newspaper blog posts have 4 topics: **atheism, religion, computer graphics, and space science**. Of course, we will *not use* class label information for topic modeling.\n",
    "\n",
    "Download the data and set up the data (**news_corpus**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "dataset = fetch_20newsgroups(shuffle=True, \n",
    "                             random_state=1, \n",
    "                             categories = categories, \n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "news_corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's normalize the corpus and create the Bag-of-Words representation of the data. We'll limit the number of features to **1000 most frequent features** to compute the topic model faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "normalized_corpus_news = normalize_corpus(news_corpus)\n",
    "\n",
    "#define a Bag-of-Words vecgtorizer\n",
    "bow_vectorizer_news = CountVectorizer(max_features=1000)\n",
    "\n",
    "#vectorize data\n",
    "bow_news_corpus = bow_vectorizer_news.fit_transform(normalized_corpus_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the topic model. We need to **set the number of topics** first. We are *lucky to know* that there are **4 topics** (atheism, religion, computer graphics, and space science) and it will allow us to judge the performance of the topic model better.\n",
    "\n",
    "**Note**: It will take a couple of minutes for the estimation to finish. The larger the number of iterations (max_iter) you allow for, the longer it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_news = LatentDirichletAllocation(n_components=4, max_iter=100,\n",
    "                                     doc_topic_prior = 0.25,\n",
    "                                     topic_word_prior = 0.25).fit(bow_news_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results with top 10 words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "space nasa launch satellite orbit mission earth year system use\n",
      "Topic 1:\n",
      "think people like know could good time well thing take\n",
      "Topic 2:\n",
      "god jesus christian believe religion bible atheist argument belief atheism\n",
      "Topic 3:\n",
      "image file use edu program software graphic format jpeg data\n"
     ]
    }
   ],
   "source": [
    "no_top_words_news = 10\n",
    "display_topics(lda_news, bow_vectorizer_news.get_feature_names(), no_top_words_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display **word vectors** (words are in alphabetical order) for each topic. Each column is a topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3d</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.005050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.001232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accept</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.000295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceptable</th>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access</th>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act</th>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action</th>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Topic_0   Topic_1   Topic_2   Topic_3\n",
       "3d          0.000010  0.000006  0.000010  0.005050\n",
       "able        0.000273  0.002062  0.000010  0.001274\n",
       "ac          0.000412  0.000007  0.000044  0.001232\n",
       "accept      0.000011  0.001713  0.002364  0.000295\n",
       "acceptable  0.000315  0.000729  0.000145  0.000074\n",
       "access      0.000956  0.000197  0.000011  0.001925\n",
       "accord      0.000318  0.000771  0.001700  0.000009\n",
       "account     0.000102  0.000421  0.001012  0.000055\n",
       "act         0.000929  0.000439  0.003240  0.000010\n",
       "action      0.000124  0.002170  0.000370  0.000038"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weights = lda_news.components_ / lda_news.components_.sum(axis=1)[:, np.newaxis]\n",
    "word_weights_df = pd.DataFrame(word_weights.T, \n",
    "                               index = bow_vectorizer_news.get_feature_names(), \n",
    "                               columns = [\"Topic_\" + str(i) for i in range(4)])\n",
    "word_weights_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **sort by word weights in Topic 0** (descending order) and see the weights by 10 most frequent words in Topic 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasa</th>\n",
       "      <td>0.016677</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>launch</th>\n",
       "      <td>0.016113</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satellite</th>\n",
       "      <td>0.011472</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orbit</th>\n",
       "      <td>0.010221</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mission</th>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earth</th>\n",
       "      <td>0.009652</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.009609</td>\n",
       "      <td>0.005065</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>0.008870</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.007598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>0.016321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Topic_0   Topic_1   Topic_2   Topic_3\n",
       "space      0.041186  0.000007  0.000010  0.000869\n",
       "nasa       0.016677  0.000007  0.000010  0.000009\n",
       "launch     0.016113  0.000007  0.000010  0.000009\n",
       "satellite  0.011472  0.000006  0.000010  0.000009\n",
       "orbit      0.010221  0.000007  0.000010  0.000009\n",
       "mission    0.009662  0.000007  0.000123  0.000009\n",
       "earth      0.009652  0.000414  0.001010  0.000009\n",
       "year       0.009609  0.005065  0.000331  0.000009\n",
       "system     0.008870  0.005121  0.000010  0.007598\n",
       "use        0.008455  0.006959  0.005839  0.016321"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weights_df.sort_values(by='Topic_0',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model Visualization\n",
    "\n",
    "You can **visualize** the topics: topic size, frequency of words in a topic and so on.\n",
    "\n",
    "In this visualization, you can rank words in a topic by **relevancy**: do you want rare and exclusive terms (i.e. found mostly in that topic) or terms that are used frequently in that topic, but not always exclusive to that topic? Relevancy parameter is λ (0 ≤ λ ≤ 1). You can adjust it:\n",
    "\n",
    "* small λ highlights potentially rare, but exclusive terms for the selected topic;\n",
    "* large values of λ (near 1) highlight frequent, but not necessarily exclusive, terms for the selected topic;\n",
    "\n",
    "Relevancy is measured as: \n",
    "\n",
    "    Relevancy = λ log[p(term | topic)] + (1 - λ) log[p(term | topic)/p(term)], \n",
    "   \n",
    "   where p(term | topic) stands for word weight in a topic and p(term) stands for word's weight in a corpus.\n",
    "\n",
    "Additional information on how to use this visualization:\n",
    "* http://www.kennyshirley.com/LDAvis/\n",
    "* https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "\n",
    "We installed all the **pyLDAvis** module required for this visualization in Session Prep. Now let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el129581404474204266086536880672\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el129581404474204266086536880672_data = {\"mdsDat\": {\"x\": [269.5624084472656, 1.782994270324707, 220.81143188476562, 50.533992767333984], \"y\": [-124.80339813232422, -295.0796813964844, -343.8321228027344, -76.05093383789062], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [32.88306805122331, 24.432746908112488, 21.49871163520923, 21.185473405454967]}, \"tinfo\": {\"Term\": [\"space\", \"god\", \"image\", \"file\", \"nasa\", \"jesus\", \"launch\", \"christian\", \"edu\", \"think\", \"religion\", \"satellite\", \"bible\", \"program\", \"software\", \"graphic\", \"format\", \"atheist\", \"orbit\", \"jpeg\", \"believe\", \"mission\", \"ftp\", \"like\", \"color\", \"people\", \"data\", \"belief\", \"argument\", \"shuttle\", \"morality\", \"muslim\", \"war\", \"gay\", \"sex\", \"islamic\", \"thread\", \"homosexuality\", \"innocent\", \"decision\", \"theory\", \"allah\", \"punishment\", \"behavior\", \"eat\", \"seriously\", \"consequence\", \"maybe\", \"moral\", \"guy\", \"stay\", \"animal\", \"yes\", \"murder\", \"exactly\", \"big\", \"guess\", \"enough\", \"happen\", \"objective\", \"think\", \"actually\", \"like\", \"anything\", \"mind\", \"sort\", \"kill\", \"could\", \"idea\", \"people\", \"really\", \"sure\", \"seem\", \"right\", \"little\", \"want\", \"human\", \"problem\", \"thing\", \"might\", \"know\", \"well\", \"much\", \"good\", \"even\", \"take\", \"long\", \"way\", \"time\", \"give\", \"us\", \"point\", \"something\", \"still\", \"question\", \"mean\", \"use\", \"year\", \"look\", \"post\", \"system\", \"format\", \"jpeg\", \"ftp\", \"color\", \"gif\", \"pub\", \"file\", \"3d\", \"graphics\", \"directory\", \"software\", \"mac\", \"pc\", \"graphic\", \"server\", \"screen\", \"unix\", \"hi\", \"driver\", \"pixel\", \"anonymous\", \"sgi\", \"comp\", \"amiga\", \"polygon\", \"viewer\", \"tiff\", \"internet\", \"animation\", \"disk\", \"image\", \"edu\", \"display\", \"package\", \"mail\", \"email\", \"info\", \"thanks\", \"window\", \"available\", \"version\", \"program\", \"computer\", \"user\", \"send\", \"faq\", \"use\", \"data\", \"line\", \"bit\", \"code\", \"information\", \"system\", \"please\", \"write\", \"know\", \"include\", \"jesus\", \"christian\", \"god\", \"religion\", \"bible\", \"atheist\", \"belief\", \"atheism\", \"faith\", \"islam\", \"church\", \"christ\", \"christianity\", \"koresh\", \"verse\", \"fallacy\", \"mormon\", \"jew\", \"quran\", \"lord\", \"juda\", \"prophecy\", \"prophet\", \"tyre\", \"contradiction\", \"p2\", \"gospel\", \"den\", \"father\", \"p3\", \"ra\", \"matthew\", \"interpretation\", \"conclusion\", \"religious\", \"argument\", \"context\", \"sin\", \"men\", \"believe\", \"statement\", \"law\", \"word\", \"true\", \"evidence\", \"quote\", \"exist\", \"many\", \"john\", \"child\", \"people\", \"book\", \"example\", \"write\", \"may\", \"know\", \"life\", \"mean\", \"use\", \"even\", \"point\", \"nasa\", \"launch\", \"satellite\", \"orbit\", \"shuttle\", \"lunar\", \"moon\", \"rocket\", \"flight\", \"station\", \"probe\", \"spacecraft\", \"solar\", \"dc\", \"vehicle\", \"fund\", \"planetary\", \"orbital\", \"payload\", \"telescope\", \"orbiter\", \"propulsion\", \"astronaut\", \"russian\", \"venus\", \"km\", \"radio\", \"atmosphere\", \"fuel\", \"titan\", \"space\", \"mission\", \"national\", \"star\", \"development\", \"technology\", \"mar\", \"earth\", \"sci\", \"report\", \"service\", \"center\", \"research\", \"year\", \"cost\", \"system\", \"data\", \"first\", \"program\", \"use\", \"new\", \"design\", \"science\", \"include\", \"high\", \"time\"], \"Freq\": [1043.0, 788.0, 845.0, 546.0, 413.0, 410.0, 399.0, 367.0, 435.0, 750.0, 302.0, 284.0, 286.0, 532.0, 293.0, 292.0, 281.0, 256.0, 253.0, 264.0, 414.0, 242.0, 255.0, 684.0, 246.0, 786.0, 443.0, 208.0, 248.0, 201.0, 121.39321844148671, 85.90072075300053, 82.96679816273725, 72.14263062905013, 61.30988333615939, 63.2562249959676, 50.461223053628416, 48.50466511668321, 47.50772109187855, 46.51135558211926, 106.53655170941472, 45.50338973810135, 43.56612791660404, 42.57907499278616, 40.61062226536189, 40.606955108485934, 40.603728633197626, 117.00207669706764, 166.7385498127785, 56.81427446932016, 51.03251029924932, 56.366440150321566, 131.79004104393533, 43.70993927510036, 70.33323442871706, 139.1808708020542, 72.22270760429818, 143.60146435200016, 104.46585652122947, 122.10804916742626, 653.2584816296063, 170.01622395418838, 548.0633858153974, 154.99852622996013, 113.4164002365419, 104.2362838969061, 127.92284121620129, 407.0540610714037, 169.73219154665222, 555.0049767354137, 204.87293713938035, 151.96741662886512, 249.27725985408225, 225.6186997268593, 158.1257517368694, 273.4926064928829, 156.23955786332075, 231.82277686146156, 334.04326662224554, 205.64799385015664, 481.8493392672438, 335.36641375948767, 275.2411718568463, 366.858042127068, 294.75307089558896, 299.2164268322208, 182.85139507757623, 260.95001869414455, 335.75414440208135, 252.59736366149488, 220.44568262158876, 256.8471752978143, 197.06056845617601, 176.82558347366597, 202.93097752327716, 197.5720411888755, 266.97489671362246, 194.34336400430846, 192.58877564296773, 188.6662570546352, 196.46911570645923, 280.7351327497049, 263.8857062882969, 254.96041675172836, 246.018806088513, 196.4831750453177, 196.48103117488535, 543.4590100580817, 143.95838970726302, 142.9690080146072, 121.16119606492852, 291.53812413272925, 113.21980887976866, 113.21894375930586, 290.34661021255516, 111.20315638325842, 92.40698156240539, 88.4512555242343, 83.48727479099959, 81.49340080942797, 79.52422861053141, 80.49038839286047, 75.56210756136596, 73.58665967030952, 73.58551131960176, 71.60311380561316, 70.61428035157832, 68.63027858995655, 66.63041785106877, 64.66755056094308, 67.59618087961617, 813.9807117559378, 420.25722032844425, 180.17759695513953, 178.97411956127206, 230.62855563812886, 136.82028983914077, 128.5053549472365, 173.1957969628066, 101.95424551235246, 251.55400566062121, 197.41968996671355, 359.28249150600067, 156.59677306607352, 141.95882552870316, 189.42805339159057, 131.91524055578327, 465.25707365587306, 255.932393442965, 186.6256607119145, 186.2801785552741, 148.06724929169604, 175.16968237500092, 216.5894998283153, 162.01450270912537, 162.19863314891867, 165.14612013293598, 155.11030821673762, 409.53837792348486, 366.7097743639852, 786.9227925482255, 301.9714774601028, 286.04959257029793, 256.1700224894612, 207.36211457633323, 199.40354264556936, 138.66483763570932, 120.7334541431105, 112.75199161701052, 98.8341779968104, 85.87979493060665, 75.91305612372018, 72.93778162235031, 69.95552334016463, 67.9634505297065, 67.94649075447396, 65.96446012353599, 62.97225070859397, 59.996732011222505, 59.9942443043152, 58.99615594173331, 58.994306818027056, 57.97897697515822, 53.02602031538303, 53.02120574673312, 52.03314375015209, 53.014486434132884, 51.03680917858753, 94.76162054891195, 122.6889584128743, 84.79593306826344, 108.60039387762801, 150.17740996708207, 225.55509347491585, 98.69796486326412, 67.8636765061059, 83.97064361897729, 304.62023228359675, 118.44375669299755, 160.54359628391552, 188.40561261595718, 179.84355925356977, 136.25739872315563, 107.48149426983082, 152.76250631640733, 194.24960949715404, 102.33488152993438, 108.42705417463122, 197.2808160515012, 136.7789666528168, 124.96849852749556, 132.38147786121266, 140.2839125059184, 156.85283047535938, 124.60307140649994, 128.56640212463998, 146.45524308380794, 128.75301637688722, 123.62168239970727, 412.2333364821356, 398.2819588537628, 283.5599566280039, 252.63513971943945, 200.7539155882443, 181.80741416813404, 182.781050710693, 153.87443324234388, 135.9133536733444, 129.91233736268964, 122.94655297428001, 114.96753691893808, 109.97858153092677, 109.97828304150731, 106.98398578996554, 88.0190434676099, 88.0091800353967, 77.05506894417191, 72.06822547384955, 69.05221972835392, 65.08915524709572, 63.0901804420239, 62.09421943067181, 66.0277538030597, 61.09401409422384, 59.1012912865461, 59.082693162701844, 55.093213168700224, 53.11445085128231, 51.12306154630653, 1018.0532011394231, 238.82423907218302, 127.4052203554883, 99.87745683324731, 114.09249976664955, 158.97882967272972, 104.68340898522342, 238.58873338632793, 93.98381930935264, 114.46387270653453, 103.37923141270721, 161.84918168825538, 139.8846584244314, 237.530008327609, 142.2436164139468, 219.23992276803955, 182.98768947360122, 161.36794743361233, 172.57442757754566, 208.9958544798852, 150.26734445794753, 119.7764457513876, 125.93665389475879, 134.22192412401043, 125.30334070382072, 138.8679704022494], \"Total\": [1043.0, 788.0, 845.0, 546.0, 413.0, 410.0, 399.0, 367.0, 435.0, 750.0, 302.0, 284.0, 286.0, 532.0, 293.0, 292.0, 281.0, 256.0, 253.0, 264.0, 414.0, 242.0, 255.0, 684.0, 246.0, 786.0, 443.0, 208.0, 248.0, 201.0, 122.15736298172298, 86.6948346080923, 83.73936237263038, 72.90336234150594, 62.0674547626013, 64.03787441540798, 51.231706377551426, 49.26141734108651, 48.27647239607086, 47.29150358628342, 108.37289593563513, 46.3066793554717, 44.3361646572787, 43.351068494098314, 41.380931968743134, 41.38097656023218, 41.380990030574566, 120.2021888663754, 171.4540634395627, 59.12178691365701, 53.21964245479438, 59.12982598770364, 138.97307591270308, 46.32628962302464, 74.89684648799899, 149.7999291486476, 77.87530664942916, 155.78101937017138, 113.3797552720191, 134.11951046833093, 750.6611695420822, 189.2850687229577, 684.5775797744823, 176.5615864623388, 126.21026671276732, 115.32543779462169, 144.990523340226, 521.1656949061029, 203.24625687233393, 786.4895060395098, 253.69668910418227, 181.4645752494694, 323.8042021205367, 290.2463751884965, 194.47995060224858, 373.0299883968688, 192.44624304929863, 311.92067762752566, 487.1659133228474, 272.53795468471196, 813.5477928333679, 518.9093853119117, 401.0304943848898, 600.0177396476203, 455.71261238667165, 481.66772433110935, 240.07844458515214, 433.21092493976295, 652.4223840430095, 427.22695410884853, 362.1444973822522, 490.633940803803, 298.510713365677, 246.0099283244943, 339.11038012718427, 363.23678744155757, 1087.6830679331886, 440.44839290835415, 421.46124416236853, 396.8095415311904, 632.559278127215, 281.4842304800571, 264.634894843164, 255.7147066282698, 246.7944909616432, 197.23760379666683, 197.23762730108731, 546.126812036246, 144.70729346204376, 143.71615628677083, 121.91116501573667, 293.38299713339904, 113.98204636974162, 113.9821359029876, 292.39283021379396, 111.9999931799763, 93.16816117389068, 89.20365543177404, 84.24793088287726, 82.2656580684285, 80.28345659405875, 81.27466100127891, 76.31880713846078, 74.33654871953408, 74.33657626307662, 72.3542552100854, 71.36315669995443, 69.38085443511316, 67.39872507155476, 65.41632208057227, 68.3898113090644, 845.6337164903966, 435.1774008908203, 185.37049774720128, 184.38297338465534, 241.88841804633978, 141.7090736328038, 132.83046034670105, 185.3042893053037, 106.071726163548, 315.5928817849398, 249.03068799355404, 532.3674352925182, 192.42870841905568, 170.65541921138822, 264.0876328021029, 156.71476623522508, 1087.6830679331886, 443.21026387458727, 274.29595914777155, 284.06524730601484, 191.0313665763086, 294.01645694207554, 632.559278127215, 290.994001292219, 346.51256151088614, 813.5477928333679, 381.5124937176605, 410.2865910129697, 367.4650066837502, 788.7045528620022, 302.73469729005166, 286.8011975837514, 256.92567038581, 208.12897734036608, 200.1622450621999, 139.41545533545278, 121.49009196463288, 113.52320872301114, 99.58149860211212, 86.63537006206985, 76.6767872471135, 73.68937873048317, 70.70186519512872, 68.71016565706229, 68.70998780332664, 66.718393291883, 63.73083496685144, 60.743365319095766, 60.7433453329932, 59.74746922374293, 59.74746690490096, 58.751508237163954, 53.772434574492905, 53.772382804989725, 52.776601031737236, 53.77234019283117, 51.78074856081071, 96.58895571731676, 125.47713510533713, 86.6371692435954, 111.53541209860315, 158.256479361643, 248.71409980534168, 104.53634843155994, 69.70086331258628, 88.63389254921765, 414.25720292101903, 136.24199376780763, 213.53314195003324, 276.3636847522613, 281.8251111815109, 196.52416511493684, 139.11697231773735, 244.29609483335435, 477.809937881652, 138.37200721797356, 161.7453673843451, 786.4895060395098, 302.9177839723227, 265.83429555670153, 346.51256151088614, 467.13433612332426, 813.5477928333679, 292.1579204119563, 363.23678744155757, 1087.6830679331886, 455.71261238667165, 490.633940803803, 413.00185243269885, 399.035529253377, 284.3113198408471, 253.3856539682105, 201.51030935256807, 182.55592492152994, 183.55332808003158, 154.62306500480855, 136.6661945889889, 130.680469719149, 123.69739819640117, 115.71658081377439, 110.72857004691572, 110.72856237010726, 107.73574006351947, 88.78115382603183, 88.781136926028, 77.80767531223816, 72.81968403162413, 69.82673581906437, 65.83650200444576, 63.841282410042965, 62.84367054643352, 66.83373891801394, 61.846057617384844, 59.850865630684126, 59.850741957811735, 55.860301710947084, 53.86523093888385, 51.870075012623474, 1043.3264772342911, 242.40697397455835, 129.67310749313737, 103.7397496377133, 119.6754656670324, 173.48775097207638, 111.69018367843825, 280.07778485001853, 101.70466972841976, 129.64337309866335, 117.63145802342443, 213.1533114449385, 192.26370112064436, 440.44839290835415, 198.94443832695916, 632.559278127215, 443.21026387458727, 411.89952836913903, 532.3674352925182, 1087.6830679331886, 377.38594645797116, 177.09915211902683, 237.03675739710056, 381.5124937176605, 246.27564977884794, 652.4223840430095], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.7559, -6.1018, -6.1365, -6.2763, -6.439, -6.4078, -6.6337, -6.6733, -6.6941, -6.7152, -5.8865, -6.7372, -6.7807, -6.8036, -6.8509, -6.851, -6.8511, -5.7928, -5.4385, -6.5152, -6.6225, -6.5231, -5.6737, -6.7774, -6.3017, -5.6192, -6.2752, -5.5879, -5.9061, -5.75, -4.073, -5.419, -4.2486, -5.5115, -5.8239, -5.9083, -5.7035, -4.546, -5.4207, -4.236, -5.2326, -5.5313, -5.0364, -5.1361, -5.4916, -4.9437, -5.5036, -5.109, -4.7437, -5.2288, -4.3773, -4.7397, -4.9373, -4.65, -4.8688, -4.8538, -5.3463, -4.9906, -4.7386, -5.0231, -5.1593, -5.0065, -5.2714, -5.3798, -5.2421, -5.2688, -4.9678, -5.2853, -5.2944, -5.315, -5.2744, -4.6205, -4.6824, -4.7168, -4.7525, -4.9773, -4.9773, -3.96, -5.2884, -5.2953, -5.4608, -4.5827, -5.5286, -5.5286, -4.5868, -5.5466, -5.7317, -5.7755, -5.8332, -5.8574, -5.8818, -5.8698, -5.933, -5.9594, -5.9595, -5.9868, -6.0007, -6.0292, -6.0587, -6.0887, -6.0444, -3.556, -4.217, -5.064, -5.0707, -4.8171, -5.3392, -5.4019, -5.1035, -5.6334, -4.7303, -4.9726, -4.3738, -5.2042, -5.3024, -5.0139, -5.3758, -4.1153, -4.713, -5.0288, -5.0307, -5.2602, -5.0922, -4.8799, -5.1702, -5.1691, -5.1511, -5.2138, -4.1149, -4.2254, -3.4618, -4.4196, -4.4738, -4.5841, -4.7955, -4.8346, -5.1979, -5.3364, -5.4048, -5.5365, -5.677, -5.8004, -5.8404, -5.8821, -5.911, -5.9113, -5.9409, -5.9873, -6.0357, -6.0357, -6.0525, -6.0525, -6.0699, -6.1592, -6.1593, -6.1781, -6.1594, -6.1974, -5.5786, -5.3203, -5.6897, -5.4423, -5.1182, -4.7114, -5.5379, -5.9125, -5.6995, -4.4109, -5.3555, -5.0514, -4.8914, -4.9379, -5.2154, -5.4527, -5.1011, -4.8608, -5.5017, -5.4439, -4.8454, -5.2116, -5.3019, -5.2443, -5.1863, -5.0747, -5.3048, -5.2735, -5.1433, -5.2721, -5.3128, -4.0937, -4.1281, -4.4679, -4.5834, -4.8132, -4.9124, -4.907, -5.0792, -5.2033, -5.2484, -5.3036, -5.3707, -5.415, -5.415, -5.4426, -5.6377, -5.6379, -5.7708, -5.8377, -5.8804, -5.9395, -5.9707, -5.9866, -5.9252, -6.0029, -6.036, -6.0364, -6.1063, -6.1429, -6.1811, -3.1897, -4.6396, -5.2679, -5.5114, -5.3783, -5.0465, -5.4644, -4.6406, -5.5722, -5.375, -5.4769, -5.0286, -5.1745, -4.645, -5.1578, -4.7251, -4.9059, -5.0316, -4.9645, -4.773, -5.1029, -5.3297, -5.2795, -5.2158, -5.2846, -5.1818], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1059, 1.103, 1.1029, 1.1017, 1.0999, 1.0999, 1.0971, 1.0967, 1.0962, 1.0956, 1.0951, 1.0947, 1.0947, 1.0942, 1.0934, 1.0933, 1.0933, 1.0852, 1.0843, 1.0724, 1.0702, 1.0644, 1.0591, 1.0541, 1.0493, 1.0387, 1.0369, 1.0308, 1.0303, 1.0184, 0.9732, 1.0049, 0.8898, 0.982, 1.0053, 1.0111, 0.987, 0.8651, 0.932, 0.7636, 0.8985, 0.9348, 0.8506, 0.8603, 0.9053, 0.8018, 0.9038, 0.8154, 0.7349, 0.8306, 0.5884, 0.6757, 0.7358, 0.6202, 0.6765, 0.6361, 0.8399, 0.6053, 0.4479, 0.5867, 0.6158, 0.465, 0.6969, 0.782, 0.5988, 0.5033, -0.2924, 0.294, 0.329, 0.3687, -0.0571, 1.4066, 1.4064, 1.4063, 1.4061, 1.4054, 1.4054, 1.4043, 1.4041, 1.404, 1.4031, 1.4029, 1.4025, 1.4025, 1.4022, 1.4021, 1.401, 1.4008, 1.4002, 1.3998, 1.3997, 1.3995, 1.3993, 1.3991, 1.3991, 1.3988, 1.3987, 1.3984, 1.3978, 1.3977, 1.3976, 1.3711, 1.3744, 1.3808, 1.3795, 1.3616, 1.3741, 1.3761, 1.3417, 1.3697, 1.1825, 1.177, 1.016, 1.2032, 1.2251, 1.077, 1.237, 0.56, 0.8601, 1.0241, 0.9873, 1.1545, 0.8914, 0.3375, 0.8236, 0.6501, -0.1853, 0.5092, 1.5354, 1.5351, 1.5349, 1.5347, 1.5346, 1.5342, 1.5335, 1.5334, 1.5318, 1.5309, 1.5304, 1.5296, 1.5284, 1.5272, 1.5269, 1.5266, 1.5263, 1.526, 1.5258, 1.5252, 1.5248, 1.5248, 1.5245, 1.5245, 1.5239, 1.5232, 1.5231, 1.523, 1.523, 1.5227, 1.5181, 1.5147, 1.5157, 1.5105, 1.4848, 1.4394, 1.4797, 1.5105, 1.4831, 1.2298, 1.3972, 1.252, 1.1541, 1.088, 1.1709, 1.2792, 1.0677, 0.6371, 1.2355, 1.1372, 0.1542, 0.7421, 0.7824, 0.5749, 0.3342, -0.1089, 0.685, 0.4986, -0.4679, 0.2732, 0.1587, 1.55, 1.55, 1.5492, 1.5489, 1.5481, 1.5477, 1.5476, 1.547, 1.5463, 1.546, 1.5458, 1.5454, 1.5451, 1.5451, 1.5449, 1.5432, 1.5431, 1.5421, 1.5415, 1.5407, 1.5404, 1.54, 1.5399, 1.5397, 1.5396, 1.5393, 1.5389, 1.538, 1.5378, 1.5373, 1.5273, 1.537, 1.5342, 1.5139, 1.5041, 1.4645, 1.4871, 1.3915, 1.4729, 1.4273, 1.4227, 1.2765, 1.2338, 0.9344, 1.2164, 0.4922, 0.6672, 0.6148, 0.4253, -0.0976, 0.631, 1.1608, 0.9194, 0.5072, 0.8761, 0.0047]}, \"token.table\": {\"Topic\": [2, 1, 2, 3, 1, 2, 1, 2, 3, 2, 2, 1, 3, 1, 3, 4, 3, 3, 4, 2, 4, 1, 3, 1, 3, 4, 3, 1, 2, 1, 2, 3, 1, 2, 3, 4, 2, 3, 4, 1, 3, 3, 3, 3, 3, 1, 2, 2, 2, 1, 2, 3, 4, 3, 4, 1, 1, 3, 4, 3, 1, 2, 4, 1, 2, 3, 4, 1, 2, 4, 4, 1, 3, 1, 2, 4, 2, 4, 2, 2, 2, 3, 4, 2, 1, 3, 4, 1, 2, 3, 1, 2, 1, 2, 4, 1, 2, 3, 1, 3, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 3, 3, 2, 3, 3, 2, 3, 1, 2, 3, 4, 4, 2, 2, 4, 4, 1, 2, 1, 2, 3, 4, 2, 3, 1, 2, 3, 4, 3, 2, 3, 2, 1, 2, 3, 1, 2, 1, 3, 2, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 3, 3, 2, 3, 4, 2, 3, 1, 3, 4, 1, 2, 3, 4, 3, 4, 1, 3, 1, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 3, 4, 1, 3, 4, 1, 2, 3, 4, 3, 4, 2, 2, 3, 1, 2, 3, 4, 2, 3, 4, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 1, 2, 3, 3, 4, 4, 1, 3, 1, 3, 1, 2, 3, 4, 1, 3, 1, 4, 2, 4, 1, 2, 3, 4, 1, 4, 4, 4, 4, 3, 3, 2, 4, 4, 2, 1, 2, 3, 4, 2, 4, 1, 2, 3, 1, 2, 3, 4, 2, 1, 2, 3, 4, 4, 1, 2, 3, 4, 2, 4, 3, 3, 4, 2, 1, 1, 2, 3, 4, 1, 2, 3, 3, 2, 3, 4, 1, 3, 4, 3, 1, 3, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 4, 4, 4, 2, 4, 1, 2, 4, 2, 1, 2, 3, 2, 3, 4, 1, 2, 2, 3, 4, 1, 2, 4, 2, 3, 2, 3, 4, 1, 2, 3, 1, 2, 2, 4, 4, 3, 4, 1, 3, 4, 1, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 4, 1, 2, 3, 4, 2, 4, 4, 1, 2, 3, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 4, 1, 2, 3, 4, 3, 2, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 4, 4, 3, 2, 3, 4, 2, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 3], \"Freq\": [0.9951122473157907, 0.8981162705908736, 0.06867947951577269, 0.03169822131497201, 0.9933772112416551, 0.995472265740549, 0.9470685743544298, 0.016911938827757676, 0.016911938827757676, 0.9936358072827834, 0.9843166248179263, 0.8778806483655042, 0.11893866848822959, 0.09247565786580317, 0.9086738555509355, 0.9865750911890139, 0.9941934850808716, 0.9963971276812473, 0.9845990500481223, 0.7984970971928479, 0.20279291357278675, 0.9919017337681975, 0.9945755879128748, 0.2293261271744559, 0.7362575661916743, 0.03620938850122988, 0.9972064357105154, 0.9279043107027724, 0.06675570580595484, 0.27810512109176, 0.6547791458616121, 0.06688604178156253, 0.16176006359691672, 0.24429070828922117, 0.4522679329138284, 0.14195270887076367, 0.23457294498995357, 0.004691458899799071, 0.7600163417674496, 0.3276755362894538, 0.667716187155868, 0.9941605759074228, 0.9987345551949376, 0.9926661586184183, 0.9953911739379413, 0.21985918204287594, 0.774741879579658, 0.9967807589280157, 0.9954726345878142, 0.031180378693462334, 0.8158865758122644, 0.010393459564487445, 0.14550843390282422, 0.9772680976301795, 0.017931524727159257, 0.990793114657405, 0.02869815183915768, 0.9470390106922034, 0.02869815183915768, 0.9872086988110957, 0.17090198794158815, 0.11561016831342727, 0.7137671261089857, 0.7809416544067203, 0.09785755374629664, 0.06907592029150351, 0.05180694021862763, 0.009025061750672492, 0.5776039520430395, 0.41289657509326655, 0.9934202851142232, 0.9938360262588909, 0.9852851260491325, 0.09034487070410442, 0.2315087311792676, 0.6775865302807832, 0.04177965777807184, 0.9525761973400381, 0.9925259920564367, 0.9943001552189875, 0.9710283037890673, 0.005394601687717041, 0.021578406750868164, 0.9846149888283187, 0.057126987092417876, 0.08926091733190293, 0.853334369692992, 0.9907945048451091, 0.9651236464491223, 0.03217078821497074, 0.028226844601107123, 0.966769427587919, 0.924374487868917, 0.012838534553734958, 0.06419267276867478, 0.647337800143422, 0.07021969357487967, 0.2830731397237337, 0.30530596562976925, 0.6920268554274769, 0.9346187894735511, 0.053406787969917206, 0.3498419938828526, 0.06771135365474566, 0.4702177337135115, 0.10909051422153468, 0.2578837784655347, 0.024560359853860445, 0.6262891762734414, 0.0900546527974883, 0.9970200195204102, 0.9900728899698522, 0.8422945914482058, 0.15314447117240107, 0.9856368499109113, 0.9942745677975642, 0.0036621531042267557, 0.3228942760060825, 0.10439438998692893, 0.17965546183797074, 0.39087201832315255, 0.9951253886084088, 0.998279724305581, 0.9972050624788321, 0.9839371163215553, 0.9912013553286038, 0.9876087698496777, 0.9937253151891732, 0.5921911002261829, 0.12873719570134412, 0.22236424712050348, 0.05617623085149562, 0.0012679018985896076, 0.9978387941900211, 0.6116485826161283, 0.17999467826305682, 0.14666233043656482, 0.059998226087685604, 0.985636068838704, 0.9918163854700393, 0.00684011300324165, 0.9950168700216153, 0.9245549468475547, 0.02568208185687652, 0.05136416371375304, 0.9641115902542708, 0.033828476851027044, 0.9172713395833734, 0.07055933381410565, 0.9851874002150611, 0.2517504270344028, 0.20302453793097003, 0.0365444168275746, 0.507561344827425, 0.9946932639132882, 0.8106159804846788, 0.05196256285158197, 0.09872886941800574, 0.03637379399610738, 0.8364237679751364, 0.07380209717427674, 0.019680559246473796, 0.07380209717427674, 0.9625917038624182, 0.005912725453700357, 0.03192871744998193, 0.09436126101453944, 0.40627765159037815, 0.14678418380039468, 0.35123358266523014, 0.9711627864820829, 0.03011357477463823, 0.02720936128271244, 0.5952047780593347, 0.05101755240508583, 0.3265123353925493, 0.9942731441973925, 0.9940840858468546, 0.9811031540170454, 0.011542390047259357, 0.9959659923150314, 0.9837928034794631, 0.999301485792499, 0.9896668908549527, 0.12285721904156796, 0.7371433142494077, 0.13731100951704653, 0.9976008649821474, 0.9877621973166825, 0.882816318275112, 0.1172490422709133, 0.9857835701836882, 0.5924667293624186, 0.20281537415933418, 0.19298190147282102, 0.012291840858141466, 0.9911735054191518, 0.9974049196688963, 0.24352191666887946, 0.7539813189171075, 0.5168437665050565, 0.42785080008696724, 0.05818770881182755, 0.8004936419047283, 0.16944755923530747, 0.029215096419880595, 0.2296789212489273, 0.6817453694214191, 0.04739406311485801, 0.04374836595217663, 0.8124230776011582, 0.10283836425331115, 0.08227069140264892, 0.7622508564491, 0.029157136585484696, 0.2082652613248907, 0.4579305990129106, 0.36065000544021975, 0.10439868578532677, 0.07829901433899508, 0.9885324746297209, 0.9969547692206161, 0.9913842012753833, 0.9549857817324109, 0.04547551341582909, 0.33695406318626603, 0.2051024732438141, 0.40601918172755036, 0.05022917712093406, 0.04476669153302904, 0.008953338306605808, 0.94010052219361, 0.9802582749179156, 0.015939158941754725, 0.3104032154932832, 0.21193047126782782, 0.2996996563383424, 0.17767908197201726, 0.9733599787443542, 0.02495794817293216, 0.5450989735775508, 0.06056655261972786, 0.35514024036113157, 0.04129537678617808, 0.9477187290782193, 0.045129463289439016, 0.7558580243926502, 0.07338427421287866, 0.11741483874060585, 0.055038205659658994, 0.8953312828121059, 0.03169314275441083, 0.07130957119742437, 0.012375881563188288, 0.985945231200667, 0.9969854641927804, 0.9740218263119045, 0.02332986410327915, 0.9905256387869461, 0.98966432913862, 0.6857333889828044, 0.17205674123568548, 0.06982012687824918, 0.07231370283818665, 0.9497846764341676, 0.04317203074700762, 0.9919852825000091, 0.9975742180651306, 0.015423398410544338, 0.9793857990695655, 0.22523361242723516, 0.20933476919707739, 0.16693785391665666, 0.39747108075394444, 0.9096364844606806, 0.08947244109449318, 0.9984779960421165, 0.989619593324219, 0.9872942519882165, 0.9856351199158963, 0.9849220302426913, 0.9708054746821686, 0.027117471359837112, 0.9887436475106351, 0.9913834225406734, 0.7056673938280357, 0.039415656231836224, 0.2504801379894109, 0.0050858911266885454, 0.996469302567626, 0.991201544009525, 0.31959421701826934, 0.5567125070640822, 0.12371389045868492, 0.5238121104686688, 0.21604701832559883, 0.2527342478525873, 0.008152717672664107, 0.9951038786999218, 0.4762990306903798, 0.29233168021208494, 0.10080402765933964, 0.12852513526565804, 0.9943620625286405, 0.7437788407123126, 0.17953282362021336, 0.012823773115729527, 0.06411886557864763, 0.6743462807839504, 0.3249635280658034, 0.9877625223155194, 0.9874895249379718, 0.9868222820989163, 0.9937251967688799, 0.9924178227891999, 0.5986251436003354, 0.1621890783153618, 0.20937135564346707, 0.026540030997059205, 0.1797048885085067, 0.05031736878238187, 0.7691369228164086, 0.9892324551531069, 0.010353150549910303, 0.9835493022414789, 0.9857856071623737, 0.8080515387247145, 0.18131888186017983, 0.011825144469142164, 0.9975731315352078, 0.050550852845137786, 0.9478284908463335, 0.023140403773025023, 0.09256161509210009, 0.8793353433749509, 0.19244402237311928, 0.07801784690802133, 0.7281665711415324, 0.7786488284417934, 0.05512558077464024, 0.14814999833184567, 0.02067209279049009, 0.9959704265027396, 0.9875251791757947, 0.9989050037085355, 0.06882673154233705, 0.9242446807113832, 0.4556255375155628, 0.012656264930987855, 0.5315631271014899, 0.9874617985460676, 0.7689832261883659, 0.071030579125833, 0.1575025884964123, 0.7156715291610392, 0.07573243694825811, 0.2044775797602969, 0.9907934371805449, 0.9910714889208129, 0.1020135276875544, 0.0170022546145924, 0.8756161126515085, 0.982801699108105, 0.9958226923295278, 0.9974675769482582, 0.014347024591579547, 0.9755976722274092, 0.9952860351591193, 0.0034085138190380797, 0.9934202162404244, 0.6599428133712376, 0.05694937983406619, 0.28139693565068, 0.9017958395719191, 0.08671113842037684, 0.023961818803133812, 0.9757252616636088, 0.9938074491249651, 0.02891851976196969, 0.963950658732323, 0.12477797432245813, 0.8661059394147094, 0.994792873635889, 0.9582927965613489, 0.037580109669072506, 0.719483157470506, 0.06097314893817846, 0.1138165446845998, 0.10975166808872124, 0.8376290512406468, 0.12674650117457156, 0.03306430465423606, 0.3098523834482152, 0.34305085310338107, 0.3462126121181588, 0.6207598825834147, 0.04775076019872421, 0.2449821610195416, 0.0851209203542475, 0.08069733984996648, 0.9164912168674764, 0.9881601823518342, 0.048568762405557575, 0.9335995440179401, 0.016189587468519194, 0.9873317408030646, 0.009227399446757614, 0.6855980495882035, 0.06568604067910933, 0.22579576483443828, 0.02052688771222166, 0.8698997983315729, 0.011989430604876197, 0.11190135231217783, 0.00532863582438942, 0.9759581231108255, 0.9945106695757207, 0.5150037892903594, 0.13334919544125376, 0.1379474435599177, 0.21305216283142844, 0.9832258771090706, 0.3016054873309544, 0.05322449776428607, 0.6386939731714328, 0.007096599701904809, 0.9874895632631474, 0.9865066579844967, 0.6074923175424773, 0.005522657432204339, 0.2540422418813996, 0.13254377837290413, 0.24547591837331112, 0.4275142398636317, 0.13423027746255964, 0.1921515615731162, 0.8320860870178802, 0.01171952235236451, 0.15821355175692087, 0.9931708821688542, 0.9863199426126877, 0.9906448019733678, 0.7910671635983241, 0.17266948240978647, 0.032124554866937015, 0.9949111457964042, 0.731844646520895, 0.235905966644098, 0.018765247346689613, 0.010722998483822636, 0.9911706710956276, 0.6024778808066567, 0.0761753642399221, 0.27238463576699423, 0.04847523178904134, 0.6455847773858138, 0.12526271800023253, 0.1580237365541395, 0.07130339332320929, 0.9616134637304764, 0.018855165955499537, 0.018855165955499537, 0.16644733927771824, 0.028947363352646648, 0.6802630387871963, 0.12302629424874825, 0.08080515141474992, 0.46751551889962456, 0.38093857095524963, 0.06926155835549994, 0.4404602289929716, 0.018163308412081303, 0.5403584252594188, 0.9498242672768985, 0.05036946871922947], \"Term\": [\"3d\", \"actually\", \"actually\", \"actually\", \"allah\", \"amiga\", \"animal\", \"animal\", \"animal\", \"animation\", \"anonymous\", \"anything\", \"anything\", \"argument\", \"argument\", \"astronaut\", \"atheism\", \"atheist\", \"atmosphere\", \"available\", \"available\", \"behavior\", \"belief\", \"believe\", \"believe\", \"believe\", \"bible\", \"big\", \"big\", \"bit\", \"bit\", \"bit\", \"book\", \"book\", \"book\", \"book\", \"center\", \"center\", \"center\", \"child\", \"child\", \"christ\", \"christian\", \"christianity\", \"church\", \"code\", \"code\", \"color\", \"comp\", \"computer\", \"computer\", \"computer\", \"computer\", \"conclusion\", \"conclusion\", \"consequence\", \"context\", \"context\", \"context\", \"contradiction\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"could\", \"data\", \"data\", \"data\", \"dc\", \"decision\", \"den\", \"design\", \"design\", \"design\", \"development\", \"development\", \"directory\", \"disk\", \"display\", \"display\", \"display\", \"driver\", \"earth\", \"earth\", \"earth\", \"eat\", \"edu\", \"edu\", \"email\", \"email\", \"enough\", \"enough\", \"enough\", \"even\", \"even\", \"even\", \"evidence\", \"evidence\", \"exactly\", \"exactly\", \"example\", \"example\", \"example\", \"example\", \"exist\", \"exist\", \"exist\", \"exist\", \"faith\", \"fallacy\", \"faq\", \"faq\", \"father\", \"file\", \"file\", \"first\", \"first\", \"first\", \"first\", \"flight\", \"format\", \"ftp\", \"fuel\", \"fund\", \"gay\", \"gif\", \"give\", \"give\", \"give\", \"give\", \"god\", \"god\", \"good\", \"good\", \"good\", \"good\", \"gospel\", \"graphic\", \"graphic\", \"graphics\", \"guess\", \"guess\", \"guess\", \"guy\", \"guy\", \"happen\", \"happen\", \"hi\", \"high\", \"high\", \"high\", \"high\", \"homosexuality\", \"human\", \"human\", \"human\", \"human\", \"idea\", \"idea\", \"idea\", \"idea\", \"image\", \"image\", \"image\", \"include\", \"include\", \"include\", \"include\", \"info\", \"info\", \"information\", \"information\", \"information\", \"information\", \"innocent\", \"internet\", \"interpretation\", \"interpretation\", \"islam\", \"islamic\", \"jesus\", \"jew\", \"john\", \"john\", \"john\", \"jpeg\", \"juda\", \"kill\", \"kill\", \"km\", \"know\", \"know\", \"know\", \"know\", \"koresh\", \"launch\", \"law\", \"law\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"little\", \"little\", \"little\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"lord\", \"lunar\", \"mac\", \"mail\", \"mail\", \"many\", \"many\", \"many\", \"many\", \"mar\", \"mar\", \"mar\", \"matthew\", \"matthew\", \"may\", \"may\", \"may\", \"may\", \"maybe\", \"maybe\", \"mean\", \"mean\", \"mean\", \"mean\", \"men\", \"men\", \"might\", \"might\", \"might\", \"might\", \"mind\", \"mind\", \"mind\", \"mission\", \"mission\", \"moon\", \"moral\", \"moral\", \"morality\", \"mormon\", \"much\", \"much\", \"much\", \"much\", \"murder\", \"murder\", \"muslim\", \"nasa\", \"national\", \"national\", \"new\", \"new\", \"new\", \"new\", \"objective\", \"objective\", \"orbit\", \"orbital\", \"orbiter\", \"p2\", \"p3\", \"package\", \"package\", \"payload\", \"pc\", \"people\", \"people\", \"people\", \"people\", \"pixel\", \"planetary\", \"please\", \"please\", \"please\", \"point\", \"point\", \"point\", \"point\", \"polygon\", \"post\", \"post\", \"post\", \"post\", \"probe\", \"problem\", \"problem\", \"problem\", \"problem\", \"program\", \"program\", \"prophecy\", \"prophet\", \"propulsion\", \"pub\", \"punishment\", \"question\", \"question\", \"question\", \"question\", \"quote\", \"quote\", \"quote\", \"quran\", \"ra\", \"ra\", \"radio\", \"really\", \"really\", \"really\", \"religion\", \"religious\", \"religious\", \"report\", \"report\", \"report\", \"research\", \"research\", \"research\", \"right\", \"right\", \"right\", \"right\", \"rocket\", \"russian\", \"satellite\", \"sci\", \"sci\", \"science\", \"science\", \"science\", \"screen\", \"seem\", \"seem\", \"seem\", \"send\", \"send\", \"send\", \"seriously\", \"server\", \"service\", \"service\", \"service\", \"sex\", \"sgi\", \"shuttle\", \"sin\", \"sin\", \"software\", \"software\", \"solar\", \"something\", \"something\", \"something\", \"sort\", \"sort\", \"space\", \"space\", \"spacecraft\", \"star\", \"star\", \"statement\", \"statement\", \"station\", \"stay\", \"stay\", \"still\", \"still\", \"still\", \"still\", \"sure\", \"sure\", \"sure\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"technology\", \"technology\", \"telescope\", \"thanks\", \"thanks\", \"thanks\", \"theory\", \"theory\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"thread\", \"tiff\", \"time\", \"time\", \"time\", \"time\", \"titan\", \"true\", \"true\", \"true\", \"true\", \"tyre\", \"unix\", \"us\", \"us\", \"us\", \"us\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"vehicle\", \"venus\", \"verse\", \"version\", \"version\", \"version\", \"viewer\", \"want\", \"want\", \"want\", \"want\", \"war\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"window\", \"window\", \"window\", \"word\", \"word\", \"word\", \"word\", \"write\", \"write\", \"write\", \"write\", \"year\", \"year\", \"year\", \"yes\", \"yes\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 4, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el129581404474204266086536880672\", ldavis_el129581404474204266086536880672_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el129581404474204266086536880672\", ldavis_el129581404474204266086536880672_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el129581404474204266086536880672\", ldavis_el129581404474204266086536880672_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                x           y  topics  cluster       Freq\n",
       "topic                                                    \n",
       "1      269.562408 -124.803398       1        1  32.883068\n",
       "3        1.782994 -295.079681       2        1  24.432747\n",
       "2      220.811432 -343.832123       3        1  21.498712\n",
       "0       50.533993  -76.050934       4        1  21.185473, topic_info=        Term         Freq        Total Category  logprob  loglift\n",
       "846    space  1043.000000  1043.000000  Default  30.0000  30.0000\n",
       "368      god   788.000000   788.000000  Default  29.0000  29.0000\n",
       "416    image   845.000000   845.000000  Default  28.0000  28.0000\n",
       "329     file   546.000000   546.000000  Default  27.0000  27.0000\n",
       "576     nasa   413.000000   413.000000  Default  26.0000  26.0000\n",
       "..       ...          ...          ...      ...      ...      ...\n",
       "234   design   119.776446   177.099152   Topic4  -5.3297   1.1608\n",
       "787  science   125.936654   237.036757   Topic4  -5.2795   0.9194\n",
       "421  include   134.221924   381.512494   Topic4  -5.2158   0.5072\n",
       "399     high   125.303341   246.275650   Topic4  -5.2846   0.8761\n",
       "913     time   138.867970   652.422384   Topic4  -5.1818   0.0047\n",
       "\n",
       "[275 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "0         2  0.995112        3d\n",
       "11        1  0.898116  actually\n",
       "11        2  0.068679  actually\n",
       "11        3  0.031698  actually\n",
       "25        1  0.993377     allah\n",
       "...     ...       ...       ...\n",
       "996       1  0.440460      year\n",
       "996       3  0.018163      year\n",
       "996       4  0.540358      year\n",
       "997       1  0.949824       yes\n",
       "997       3  0.050369       yes\n",
       "\n",
       "[475 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 4, 3, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare to display result in the Jupyter notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\n",
    "pyLDAvis.sklearn.prepare(lda_news, bow_news_corpus, bow_vectorizer_news, mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 2:</font>**\n",
    "    \n",
    "**<font color=green>2.1. Fit a topic model with 3 topics (n_components = 3). The script is provided below. Important! Note that the model with three topics is called lda_news_3_topics.</font>**\n",
    "\n",
    "**<font color=green>2.2. Use the visualization tool to answer the following question: Which topic is the most common / largest topic in the corpus? Can you give a name to that topic? List 5 most relevant and exclusive terms for that topic (with lambda = 0.2).</font>**\n",
    "\n",
    "**<font color=green>2.3. You fit the model with 3 topics, but you know that the dataset has four classes (topics). Which 2 of the 4 classes ('atheism', 'religion','computer graphics', 'space science') were grouped together into one topic by the topic model when you fit it with 3 topics? Why?</font>**\n",
    "\n",
    "Your answer (need to add lines of code related to visualization):\n",
    "\n",
    "**Answer 2.1**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the LDA model with 3 topics\n",
    "lda_news_3_topics = LatentDirichletAllocation(n_components=3, max_iter=100,\n",
    "                                     doc_topic_prior = 0.25,\n",
    "                                     topic_word_prior = 0.25).fit(bow_news_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2.2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "god people think know like jesus good thing believe even\n",
      "Topic 1:\n",
      "image file use edu program software graphic format jpeg data\n",
      "Topic 2:\n",
      "space nasa launch year satellite orbit system use earth mission\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda_news_3_topics, bow_vectorizer_news.get_feature_names(), no_top_words_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el12958140446593315888851189957\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el12958140446593315888851189957_data = {\"mdsDat\": {\"x\": [-1015.7005615234375, -387.9960021972656, -2184.74560546875], \"y\": [1338.981201171875, -373.316162109375, -60.565799713134766], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [48.496115857098886, 27.021972823207445, 24.48191131969367]}, \"tinfo\": {\"Term\": [\"space\", \"image\", \"file\", \"launch\", \"god\", \"nasa\", \"edu\", \"satellite\", \"software\", \"graphic\", \"program\", \"format\", \"orbit\", \"jpeg\", \"ftp\", \"mission\", \"color\", \"year\", \"earth\", \"jesus\", \"mail\", \"people\", \"data\", \"shuttle\", \"available\", \"christian\", \"moon\", \"gif\", \"pub\", \"lunar\", \"jesus\", \"christian\", \"religion\", \"bible\", \"atheist\", \"god\", \"argument\", \"belief\", \"atheism\", \"evidence\", \"moral\", \"child\", \"religious\", \"faith\", \"statement\", \"morality\", \"islam\", \"church\", \"christ\", \"existence\", \"muslim\", \"christianity\", \"interpretation\", \"woman\", \"indeed\", \"koresh\", \"verse\", \"gay\", \"fallacy\", \"evil\", \"law\", \"love\", \"truth\", \"matthew\", \"believe\", \"agree\", \"people\", \"claim\", \"conclusion\", \"think\", \"die\", \"kill\", \"life\", \"true\", \"something\", \"reason\", \"even\", \"seem\", \"mean\", \"thing\", \"really\", \"fact\", \"know\", \"word\", \"us\", \"way\", \"take\", \"right\", \"many\", \"well\", \"like\", \"good\", \"could\", \"give\", \"point\", \"time\", \"use\", \"may\", \"format\", \"jpeg\", \"ftp\", \"color\", \"gif\", \"pub\", \"3d\", \"graphics\", \"file\", \"info\", \"directory\", \"mac\", \"pc\", \"screen\", \"unix\", \"graphic\", \"software\", \"hi\", \"pixel\", \"sgi\", \"comp\", \"amiga\", \"polygon\", \"viewer\", \"tiff\", \"animation\", \"internet\", \"plot\", \"postscript\", \"output\", \"image\", \"edu\", \"display\", \"server\", \"mail\", \"package\", \"window\", \"email\", \"thanks\", \"algorithm\", \"available\", \"version\", \"program\", \"computer\", \"code\", \"user\", \"send\", \"use\", \"line\", \"data\", \"bit\", \"information\", \"system\", \"please\", \"know\", \"look\", \"work\", \"write\", \"launch\", \"satellite\", \"orbit\", \"shuttle\", \"lunar\", \"moon\", \"rocket\", \"flight\", \"station\", \"probe\", \"spacecraft\", \"dc\", \"solar\", \"vehicle\", \"fund\", \"planetary\", \"orbital\", \"payload\", \"orbiter\", \"propulsion\", \"astronaut\", \"venus\", \"km\", \"radio\", \"atmosphere\", \"p2\", \"fuel\", \"den\", \"p3\", \"sky\", \"nasa\", \"mission\", \"space\", \"star\", \"mar\", \"development\", \"earth\", \"technology\", \"national\", \"year\", \"report\", \"cost\", \"center\", \"service\", \"research\", \"system\", \"first\", \"data\", \"use\", \"new\", \"time\", \"program\", \"high\", \"science\", \"include\", \"design\", \"work\"], \"Freq\": [1044.0, 847.0, 547.0, 399.0, 784.0, 413.0, 436.0, 284.0, 293.0, 292.0, 533.0, 282.0, 253.0, 265.0, 256.0, 242.0, 247.0, 442.0, 280.0, 407.0, 242.0, 788.0, 443.0, 201.0, 316.0, 365.0, 183.0, 197.0, 197.0, 182.0, 407.4826268100888, 364.8718361492486, 300.4703836875157, 284.6175577376473, 254.89157187661186, 782.9631615356919, 246.95836231780046, 206.3403502940622, 198.3944813328867, 195.41770465769213, 171.66119030377686, 160.75397389912536, 156.79898147796283, 137.97295367594288, 134.9799099500295, 122.12038485094013, 120.13886016620177, 112.2087859762692, 98.34035909625788, 92.39222681299135, 86.45048629253745, 85.4586218625401, 85.44997318370939, 79.49990190215952, 81.46244046069418, 75.5498597185172, 72.57691502135008, 72.57637207549477, 69.6063107822345, 69.60442849386608, 210.63606442171073, 131.78886004148902, 117.09690763173242, 122.54942081711856, 395.44671424945545, 150.06086268631688, 733.9253371588891, 218.24192481072606, 109.07334783015874, 680.485201535777, 121.86671896567088, 140.90618494620307, 267.8511935797381, 257.26802104633737, 269.885721404922, 186.847650251778, 388.57859755479086, 284.8577759203955, 309.3626368161675, 397.61515625377547, 224.72120592388757, 198.2623927171284, 557.4894332833508, 236.59940619279453, 290.01047448775, 330.978555020815, 357.4427951044714, 243.23038620066953, 342.94565691262403, 363.02416647801965, 436.2577918226634, 398.51796709022005, 346.90340498773867, 304.2883202967536, 315.54950602786306, 358.01061673608154, 346.0711154507509, 265.7808833730526, 281.54218140521084, 264.6435945270176, 255.6934239433741, 246.73549547343717, 197.0498083763363, 197.04554748362114, 144.3739287138651, 143.38062599436304, 545.3007156015016, 132.41064841950393, 121.51018634462977, 113.55130436809854, 113.54680550332561, 92.67946385319979, 88.70433863688048, 291.30333384151936, 292.24558571385603, 83.73597125888178, 79.74989652129763, 75.78257180790416, 73.8003977306488, 73.79825566807341, 71.81306488069673, 70.81824781689451, 68.8304708606724, 64.85536932291576, 66.82537754040295, 61.834367597067256, 56.90371254269614, 57.88885012434994, 818.1526326398332, 420.33202506967956, 180.68115884479394, 110.16316035918665, 233.09693085366655, 179.07374964881697, 104.0970839760451, 136.89911847236147, 173.41147110564827, 82.62498396669126, 256.8828941353883, 198.24229273683753, 360.8034732242354, 158.78426253940484, 156.43329964369477, 143.14920978852314, 194.54532137440557, 500.47107697643895, 197.41440341804156, 263.89164721216736, 193.97564427111476, 184.78422805206063, 230.16957077370407, 174.64869516743195, 207.61483173120558, 176.4548565287196, 171.22763691083827, 163.33985615943746, 398.7400570649654, 283.88298199100615, 252.9265810320023, 200.98307556042707, 182.01703214460764, 183.0035807665438, 154.05296186817046, 136.0718325636135, 130.0655045496526, 123.08660328042485, 115.1018487332604, 110.10708663863215, 110.10659780469442, 107.111268331513, 88.13224087915134, 88.10896921451786, 77.14680849263351, 72.15187596825764, 65.16547089069991, 63.161911529538855, 62.16833920099161, 61.16537335335419, 59.17211304540582, 59.15323893158475, 55.168104711417314, 53.18067871056761, 53.179899212594734, 52.18416706871768, 51.185197953873754, 52.17370829420213, 404.9264428785538, 238.23773046622816, 1007.8102249880599, 99.62065907548639, 106.38305273036529, 112.42142246316861, 240.56483173453435, 155.09557156149785, 118.89336384672986, 294.99475027792914, 110.8862077568756, 155.78360754875075, 163.9311155990144, 98.751922508032, 129.98123119089695, 248.71476644094875, 176.98631906335643, 177.05799768563168, 243.6712684650199, 161.8339909928, 188.20424824429128, 172.28812564733693, 136.93314730949092, 134.82925189174026, 132.5963256620035, 120.51640118968922, 122.93621909517009], \"Total\": [1044.0, 847.0, 547.0, 399.0, 784.0, 413.0, 436.0, 284.0, 293.0, 292.0, 533.0, 282.0, 253.0, 265.0, 256.0, 242.0, 247.0, 442.0, 280.0, 407.0, 242.0, 788.0, 443.0, 201.0, 316.0, 365.0, 183.0, 197.0, 197.0, 182.0, 407.98112425606865, 365.37503901822134, 300.97041663695484, 285.11697155592805, 255.39177496923892, 784.5036270321629, 247.46508252851766, 206.8406211113236, 198.91395600109533, 195.9415661932753, 172.16121863498768, 161.2620340855236, 157.29861671826555, 138.47265463066665, 135.5002892721519, 122.6192117900447, 120.63753047212131, 112.71082511953934, 98.83905164478794, 92.89403010512042, 86.94897109158424, 85.95813667803151, 85.95818816208697, 80.01320520353606, 81.99489438469026, 76.04973935109837, 73.07722513949109, 73.07722594328402, 70.10469032927621, 70.10470384820168, 212.7987943322149, 133.52822994465922, 118.66430409610432, 124.61323994262008, 413.07063512049484, 153.35734833195374, 788.6893175274222, 227.71309707793858, 110.73835074854266, 754.185525430987, 124.61869429191489, 145.44042796249337, 292.2416126829223, 281.2469545301678, 299.0795249840346, 200.00471575992515, 456.8285292995869, 324.87557813647, 363.6777885915428, 488.70133106093226, 254.59156537624295, 219.8605112110103, 816.2747378529762, 275.4820624360193, 362.94022828573935, 434.30362969899994, 483.02799267842784, 291.3170922006622, 477.9156497349715, 520.7528803486197, 688.6286737140507, 602.1430785139629, 523.9176270886772, 428.42369084322894, 491.9357458265189, 654.547040029264, 1090.2134608922097, 467.50780069116064, 282.0429499423715, 265.1452508503078, 256.1994309260447, 247.2535820698265, 197.55445540803353, 197.5544733577108, 144.87335453707917, 143.8793713121987, 547.4311128012017, 132.94560294233142, 122.01177145716407, 114.05991754578174, 114.05996368131676, 93.18626772691417, 89.2103716005808, 292.98234537544164, 293.9669752349902, 84.24041615323894, 80.26456842900538, 76.28857247648767, 74.30059600750711, 74.3006134970255, 72.31262625022141, 71.31865837656166, 69.33067570683946, 65.35475634124758, 67.3428046291008, 62.372764687661295, 57.40289278530279, 58.39690682906433, 847.7303245022506, 436.0620416411617, 185.6373125186856, 112.07862785649425, 242.25626356318915, 184.65687207112515, 106.11516381210659, 141.87734496993488, 185.61081385888693, 84.23693633415544, 316.11805604812463, 249.12178444259217, 533.3460224340286, 192.7132281829322, 191.4809968811347, 170.83417399457565, 264.31102396118615, 1090.2134608922097, 274.99785917664735, 443.9017637134801, 284.8435420589138, 294.3171982509501, 634.6032355620596, 291.61356477162644, 816.2747378529762, 422.909442793343, 418.40969709946916, 346.23854798718764, 399.2431377757267, 284.38802661065625, 253.42710234418675, 201.49257716611604, 182.5165644131918, 183.51521268690735, 154.55184363032035, 136.5745040183198, 130.5819841132519, 123.59087878800253, 115.60098027871337, 110.60727340938466, 110.60727304230883, 107.61105164628091, 88.63496586288049, 88.63485959650133, 77.64884227634295, 72.6551385668839, 65.6639818726365, 63.666473721534395, 62.66775284684957, 61.66899528462552, 59.671534581507714, 59.67142624574278, 55.67650250960669, 53.67910380386538, 53.67909196216198, 52.68037434940772, 51.68163318596179, 52.68029888376719, 413.18836166883136, 242.41169354142903, 1044.2571674853118, 103.58846675366323, 111.58351655989757, 119.56413236744915, 280.0820289126269, 173.44247162017834, 129.53477079888697, 442.0485765333133, 129.45909681676142, 199.2430861832057, 213.23411208550684, 117.5010187028632, 192.14656481970349, 634.6032355620596, 412.5175944763697, 443.9017637134801, 1090.2134608922097, 377.82885598811265, 654.547040029264, 533.3460224340286, 246.73529428387414, 237.65458810561933, 381.7943593106519, 177.21753618746519, 418.40969709946916], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.9335, -5.0439, -5.2381, -5.2923, -5.4026, -4.2804, -5.4342, -5.6139, -5.6532, -5.6683, -5.7979, -5.8636, -5.8885, -6.0164, -6.0383, -6.1385, -6.1548, -6.2231, -6.355, -6.4174, -6.4839, -6.4954, -6.4955, -6.5677, -6.5433, -6.6187, -6.6588, -6.6588, -6.7006, -6.7006, -5.5933, -6.0623, -6.1805, -6.135, -4.9635, -5.9324, -4.3451, -5.5579, -6.2514, -4.4207, -6.1405, -5.9954, -5.353, -5.3934, -5.3455, -5.7132, -4.981, -5.2915, -5.209, -4.958, -5.5286, -5.6539, -4.62, -5.4771, -5.2736, -5.1414, -5.0645, -5.4495, -5.1059, -5.049, -4.8652, -4.9557, -5.0944, -5.2255, -5.1892, -5.0629, -5.0968, -5.3608, -4.7184, -4.7803, -4.8147, -4.8503, -5.0752, -5.0752, -5.3862, -5.3931, -4.0573, -5.4727, -5.5586, -5.6264, -5.6264, -5.8295, -5.8733, -4.6843, -4.681, -5.931, -5.9797, -6.0308, -6.0573, -6.0573, -6.0846, -6.0985, -6.127, -6.1865, -6.1566, -6.2342, -6.3173, -6.3001, -3.6516, -4.3176, -5.1619, -5.6567, -4.9072, -5.1708, -5.7133, -5.4394, -5.203, -5.9443, -4.81, -5.0691, -4.4703, -5.2911, -5.306, -5.3947, -5.088, -4.1431, -5.0733, -4.7831, -5.0909, -5.1394, -4.9198, -5.1959, -5.023, -5.1856, -5.2156, -5.2628, -4.2716, -4.6114, -4.7268, -4.9567, -5.0558, -5.0504, -5.2226, -5.3467, -5.3919, -5.447, -5.5141, -5.5585, -5.5585, -5.5861, -5.7811, -5.7813, -5.9142, -5.9811, -6.083, -6.1142, -6.1301, -6.1463, -6.1795, -6.1798, -6.2495, -6.2862, -6.2862, -6.3051, -6.3245, -6.3053, -4.2562, -4.7867, -3.3444, -5.6586, -5.5929, -5.5377, -4.7769, -5.2159, -5.4817, -4.573, -5.5514, -5.2115, -5.1605, -5.6673, -5.3925, -4.7436, -5.0838, -5.0834, -4.7641, -5.1733, -5.0224, -5.1108, -5.3404, -5.3559, -5.3726, -5.4681, -5.4483], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7225, 0.7223, 0.722, 0.7219, 0.7217, 0.7217, 0.7216, 0.7213, 0.7211, 0.721, 0.7208, 0.7205, 0.7205, 0.7201, 0.7198, 0.7196, 0.7195, 0.7192, 0.7186, 0.7183, 0.7179, 0.7179, 0.7178, 0.7173, 0.7172, 0.7171, 0.7168, 0.7168, 0.7166, 0.7165, 0.7135, 0.7106, 0.7104, 0.707, 0.6801, 0.702, 0.6517, 0.6812, 0.7085, 0.6209, 0.7014, 0.692, 0.6365, 0.6346, 0.621, 0.6556, 0.5619, 0.5922, 0.5619, 0.5174, 0.5989, 0.6203, 0.3424, 0.5715, 0.4994, 0.452, 0.4226, 0.5433, 0.3918, 0.3629, 0.2672, 0.3109, 0.3114, 0.3815, 0.2797, 0.1203, -0.4238, 0.1589, 1.3067, 1.3066, 1.3065, 1.3064, 1.306, 1.3059, 1.3051, 1.305, 1.3046, 1.3045, 1.3044, 1.3041, 1.304, 1.3031, 1.3028, 1.3028, 1.3026, 1.3025, 1.3021, 1.3019, 1.3018, 1.3017, 1.3016, 1.3015, 1.3013, 1.3008, 1.3008, 1.2999, 1.2998, 1.2998, 1.273, 1.2718, 1.2815, 1.2913, 1.27, 1.2778, 1.2893, 1.2728, 1.2405, 1.2892, 1.101, 1.0801, 0.9177, 1.1149, 1.1064, 1.1317, 1.0021, 0.5299, 0.9771, 0.7885, 0.9243, 0.8431, 0.2943, 0.7959, -0.0605, 0.4344, 0.4151, 0.5572, 1.406, 1.4055, 1.4053, 1.4047, 1.4045, 1.4044, 1.404, 1.4035, 1.4033, 1.4031, 1.4029, 1.4027, 1.4027, 1.4026, 1.4015, 1.4013, 1.4007, 1.4003, 1.3996, 1.3993, 1.3992, 1.399, 1.3988, 1.3985, 1.3981, 1.3979, 1.3979, 1.3978, 1.3976, 1.3976, 1.387, 1.3899, 1.3717, 1.3682, 1.3595, 1.3456, 1.2551, 1.2954, 1.3215, 1.0028, 1.2524, 1.1612, 1.1443, 1.2334, 1.0164, 0.4705, 0.561, 0.4881, -0.0911, 0.5594, 0.1608, 0.2772, 0.8184, 0.8404, 0.3497, 1.0216, 0.1824]}, \"token.table\": {\"Topic\": [2, 1, 3, 1, 2, 2, 2, 1, 3, 1, 1, 3, 2, 3, 1, 1, 3, 1, 1, 2, 3, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 3, 1, 2, 3, 2, 3, 1, 3, 2, 1, 2, 3, 1, 3, 1, 2, 1, 2, 1, 2, 3, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 2, 3, 3, 2, 2, 3, 3, 1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 2, 3, 2, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 1, 1, 1, 2, 1, 3, 3, 1, 2, 3, 1, 3, 1, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 3, 2, 1, 2, 1, 2, 3, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 3, 3, 1, 1, 1, 2, 3, 2, 3, 1, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 1, 2, 3, 2, 3, 1, 2, 2, 1, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 1, 2, 3, 1, 3, 1, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 3, 1, 2, 3, 2, 1, 2, 1, 2, 3, 2, 3, 1, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 2, 3, 3, 1, 3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 3, 1, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.9939715999545269, 0.9781076787746322, 0.019562153575492645, 0.011871276942375351, 0.9853159862171542, 0.9959540913206923, 0.9945718359135909, 0.9981206135275102, 0.9893445541524131, 0.9954052695976229, 0.9984659843908986, 0.9878494072164471, 0.8129874111362854, 0.18663913329587875, 0.9959358993083318, 0.9562529175785552, 0.041155188857811234, 0.9995897418687856, 0.27032383968906765, 0.6810756480477808, 0.04914978903437593, 0.014069043506495812, 0.21572533376626912, 0.7691077116884377, 0.9983751036813499, 0.991510929831628, 0.9989735505215981, 0.9888534498878175, 0.993693373118461, 0.9573450222996435, 0.008782981855960032, 0.03513192742384013, 0.18278576239984212, 0.8147022552678678, 0.9989744048692695, 0.9959543257570002, 0.020756229542286623, 0.8250601243058931, 0.15567172156714967, 0.9843021795358864, 0.009030295225099875, 0.07528492098444697, 0.1455508472365975, 0.7829631782382486, 0.6623178569658384, 0.16033054750758047, 0.17750882045482125, 0.006758252039603009, 0.5947261794850649, 0.39873687033657756, 0.9945096430762108, 0.9870848611497126, 0.06771338919476963, 0.2539252094803861, 0.6827766743805938, 0.058545985835345066, 0.936735773365521, 0.9789863446508219, 0.016048956469685606, 0.9999035219551073, 0.010773696154423088, 0.9750195019752894, 0.01616054423163463, 0.13924492103763736, 0.8604622043607847, 0.03439877487053459, 0.9631656963749684, 0.035241708259056766, 0.9656228062981553, 0.8515230005368051, 0.11163926228117496, 0.039402092569826457, 0.9951946582260829, 0.9985064647241304, 0.9903758066679987, 0.9005709979904951, 0.027290030242136217, 0.06822507560534054, 0.996586657257873, 0.9985066572752196, 0.003653427715801559, 0.9955590525559248, 0.4436174418991547, 0.12847936841888086, 0.4290726077385267, 0.9957934753455686, 0.999847718433025, 0.9992215793558796, 0.9873490415478587, 0.9928361696008008, 0.9989432283137847, 0.9971934046899203, 0.7095779400099546, 0.1727262090813705, 0.11670689802795305, 0.9980833396043671, 0.0012746913660336746, 0.6626332083475867, 0.23084214526394622, 0.1079477657709101, 0.9932339084360139, 0.0034131749430790855, 0.993888134871742, 0.9971460711589838, 0.22696388112018875, 0.21885802822303915, 0.5552509234547475, 0.004718481673223878, 0.964929502174283, 0.030670130875955205, 0.23049057130882772, 0.42431218809125104, 0.34835506800084187, 0.9878663861676243, 0.9928872943414188, 0.07474928454993533, 0.6285735291699107, 0.2955994434474715, 0.9949095581779102, 0.9888528576209614, 0.9947153222581206, 0.9975951724289753, 0.9994521838507687, 0.9694690944966246, 0.027502669347421974, 0.9887461486248449, 0.6823682936275365, 0.25481616709969046, 0.06247896404848179, 0.999345962898456, 0.999391003244085, 0.9915469712229352, 0.00939854949026479, 0.9170494151726979, 0.0821238282244207, 0.6331423837588364, 0.2337979903329648, 0.13359885161883703, 0.21091073280953498, 0.7163692131634206, 0.07272783889983965, 0.43271675087525524, 0.4161647440111744, 0.14896806177672722, 0.9885550048458472, 0.007489053067013994, 0.9971697669477146, 0.9994746835955085, 0.03715074222488566, 0.9617914375998176, 0.7176998706575333, 0.21551920314205808, 0.06695742233539669, 0.044809485792787505, 0.9499610988070951, 0.9870540245694365, 0.016049658936088396, 0.5689744633281996, 0.23742919334372237, 0.19464915850701564, 0.8496532086732603, 0.08249060278381169, 0.06599248222704934, 0.01650085415255096, 0.9818008220767821, 0.9971925341808783, 0.9990635600963682, 0.9949501242015407, 0.9890858847474494, 0.01936162956693336, 0.9801824968260013, 0.07719934916568305, 0.9186722550716283, 0.3387777242827297, 0.23290968544437665, 0.4287655572953297, 0.9983146934947522, 0.9916438898852633, 0.9898881875009594, 0.9932032901977818, 0.9873488237369478, 0.9868109201675356, 0.9693654939148635, 0.027077248433376076, 0.990982901143588, 0.9994742793230734, 0.9306579710007031, 0.05452083481339269, 0.015215116692109587, 0.996703795532902, 0.9928373599350026, 0.4012159039708153, 0.6001092580760058, 0.9940235984483299, 0.6423603136809607, 0.2866227981930869, 0.0731802889003626, 0.995676740474898, 0.9929813156488527, 0.9952190744673313, 0.6768588961299573, 0.32249232724197413, 0.9895317946388954, 0.9971933140855442, 0.9887479437314324, 0.8837684770408176, 0.03535073908163271, 0.08248505785714298, 0.9349779543421601, 0.06499846741416086, 0.9967757075668823, 0.9981015934882607, 0.09269337030046643, 0.04634668515023321, 0.8574136752793144, 0.10408721081621501, 0.21858314271405152, 0.6765668703053976, 0.8341426112842671, 0.08924982672177344, 0.07551908414919291, 0.9964293947108109, 0.9986355733211388, 0.3997398104419501, 0.033662299826690535, 0.5680513095754028, 0.9980011247208652, 0.8772589236617857, 0.12312405946130325, 0.08323527210590612, 0.737767184575077, 0.1816042300492497, 0.9814538427508612, 0.01784461532274293, 0.02553169353864417, 0.12765846769322084, 0.8425458867752575, 0.9962173564516938, 0.9975553582516842, 0.9870862751696191, 0.0034017426590201975, 0.9933088564338977, 0.9945096463767211, 0.9027699238669483, 0.09696417700793149, 0.034474266608762696, 0.9652794650453556, 0.9948012527466081, 0.03861433734232336, 0.965358433558084, 0.996307836132017, 0.9955431515518468, 0.2458228878424058, 0.3624311807932906, 0.39237114790230154, 0.7390876003281035, 0.1117947630748392, 0.1490596840997856, 0.10378080888640817, 0.8936680765218481, 0.04848855415742301, 0.9320577632482423, 0.01616285138580767, 0.814403347615144, 0.11049693661109994, 0.07571086397427218, 0.9016349121940613, 0.05038548038731519, 0.04773361299850913, 0.9952304560215496, 0.5469431196022125, 0.16499960032692446, 0.28722152649501664, 0.9137876725788086, 0.07111188113453763, 0.014222376226907526, 0.985974686248053, 0.00842713407049618, 0.9976418481752022, 0.7990296401414224, 0.02204219696941855, 0.1790928503765257, 0.3173690404784034, 0.45862578103815527, 0.22380938114661977, 0.011707259462404192, 0.8370690515618998, 0.1521943730112545, 0.9943216645787512, 0.9891518374583881, 0.9989432393013873, 0.18464864525165714, 0.7947919947788721, 0.020070504918658385, 0.9955319073042689, 0.7621396123937626, 0.1220344394467354, 0.1151268296667315, 0.6970676758561345, 0.1766672897486622, 0.12481928080068523, 0.9800672803384461, 0.018847447698816272, 0.9873370251702992, 0.8603100975223867, 0.025410002880408046, 0.11616001316757964, 0.29636024418076834, 0.40869033673315636, 0.2939702422115686, 0.46788551113608157, 0.47077369330358826, 0.0606518255176402, 0.3121828851531211, 0.020359753379551374, 0.6673474718852951], \"Term\": [\"3d\", \"agree\", \"agree\", \"algorithm\", \"algorithm\", \"amiga\", \"animation\", \"argument\", \"astronaut\", \"atheism\", \"atheist\", \"atmosphere\", \"available\", \"available\", \"belief\", \"believe\", \"believe\", \"bible\", \"bit\", \"bit\", \"bit\", \"center\", \"center\", \"center\", \"child\", \"christ\", \"christian\", \"christianity\", \"church\", \"claim\", \"claim\", \"claim\", \"code\", \"code\", \"color\", \"comp\", \"computer\", \"computer\", \"computer\", \"conclusion\", \"conclusion\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"data\", \"data\", \"data\", \"dc\", \"den\", \"design\", \"design\", \"design\", \"development\", \"development\", \"die\", \"die\", \"directory\", \"display\", \"display\", \"display\", \"earth\", \"earth\", \"edu\", \"edu\", \"email\", \"email\", \"even\", \"even\", \"even\", \"evidence\", \"evil\", \"existence\", \"fact\", \"fact\", \"fact\", \"faith\", \"fallacy\", \"file\", \"file\", \"first\", \"first\", \"first\", \"flight\", \"format\", \"ftp\", \"fuel\", \"fund\", \"gay\", \"gif\", \"give\", \"give\", \"give\", \"god\", \"god\", \"good\", \"good\", \"good\", \"graphic\", \"graphic\", \"graphics\", \"hi\", \"high\", \"high\", \"high\", \"image\", \"image\", \"image\", \"include\", \"include\", \"include\", \"indeed\", \"info\", \"information\", \"information\", \"information\", \"internet\", \"interpretation\", \"islam\", \"jesus\", \"jpeg\", \"kill\", \"kill\", \"km\", \"know\", \"know\", \"know\", \"koresh\", \"launch\", \"law\", \"law\", \"life\", \"life\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"look\", \"look\", \"look\", \"love\", \"love\", \"lunar\", \"mac\", \"mail\", \"mail\", \"many\", \"many\", \"many\", \"mar\", \"mar\", \"matthew\", \"matthew\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"mission\", \"mission\", \"moon\", \"moral\", \"morality\", \"muslim\", \"nasa\", \"nasa\", \"national\", \"national\", \"new\", \"new\", \"new\", \"orbit\", \"orbital\", \"orbiter\", \"output\", \"p2\", \"p3\", \"package\", \"package\", \"payload\", \"pc\", \"people\", \"people\", \"people\", \"pixel\", \"planetary\", \"please\", \"please\", \"plot\", \"point\", \"point\", \"point\", \"polygon\", \"postscript\", \"probe\", \"program\", \"program\", \"propulsion\", \"pub\", \"radio\", \"really\", \"really\", \"really\", \"reason\", \"reason\", \"religion\", \"religious\", \"report\", \"report\", \"report\", \"research\", \"research\", \"research\", \"right\", \"right\", \"right\", \"rocket\", \"satellite\", \"science\", \"science\", \"science\", \"screen\", \"seem\", \"seem\", \"send\", \"send\", \"send\", \"server\", \"server\", \"service\", \"service\", \"service\", \"sgi\", \"shuttle\", \"sky\", \"software\", \"software\", \"solar\", \"something\", \"something\", \"space\", \"space\", \"spacecraft\", \"star\", \"star\", \"statement\", \"station\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"technology\", \"technology\", \"thanks\", \"thanks\", \"thanks\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"tiff\", \"time\", \"time\", \"time\", \"true\", \"true\", \"true\", \"truth\", \"truth\", \"unix\", \"us\", \"us\", \"us\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"vehicle\", \"venus\", \"verse\", \"version\", \"version\", \"version\", \"viewer\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"window\", \"window\", \"woman\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"write\", \"write\", \"write\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el12958140446593315888851189957\", ldavis_el12958140446593315888851189957_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el12958140446593315888851189957\", ldavis_el12958140446593315888851189957_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el12958140446593315888851189957\", ldavis_el12958140446593315888851189957_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                 x            y  topics  cluster       Freq\n",
       "topic                                                      \n",
       "0     -1015.700562  1338.981201       1        1  48.496116\n",
       "1      -387.996002  -373.316162       2        1  27.021973\n",
       "2     -2184.745605   -60.565800       3        1  24.481911, topic_info=        Term         Freq        Total Category  logprob  loglift\n",
       "846    space  1044.000000  1044.000000  Default  30.0000  30.0000\n",
       "416    image   847.000000   847.000000  Default  29.0000  29.0000\n",
       "329     file   547.000000   547.000000  Default  28.0000  28.0000\n",
       "479   launch   399.000000   399.000000  Default  27.0000  27.0000\n",
       "368      god   784.000000   784.000000  Default  26.0000  26.0000\n",
       "..       ...          ...          ...      ...      ...      ...\n",
       "399     high   136.933147   246.735294   Topic3  -5.3404   0.8184\n",
       "787  science   134.829252   237.654588   Topic3  -5.3559   0.8404\n",
       "421  include   132.596326   381.794359   Topic3  -5.3726   0.3497\n",
       "234   design   120.516401   177.217536   Topic3  -5.4681   1.0216\n",
       "989     work   122.936219   418.409697   Topic3  -5.4483   0.1824\n",
       "\n",
       "[213 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "0         2  0.993972         3d\n",
       "22        1  0.978108      agree\n",
       "22        3  0.019562      agree\n",
       "24        1  0.011871  algorithm\n",
       "24        2  0.985316  algorithm\n",
       "...     ...       ...        ...\n",
       "993       2  0.470774      write\n",
       "993       3  0.060652      write\n",
       "996       1  0.312183       year\n",
       "996       2  0.020360       year\n",
       "996       3  0.667347       year\n",
       "\n",
       "[316 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare to display result in the Jupyter notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\n",
    "pyLDAvis.sklearn.prepare(lda_news_3_topics, bow_news_corpus, bow_vectorizer_news, mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics are:\n",
    "Topic 0:\n",
    "god people think know like jesus good thing believe even\n",
    "Topic 1:\n",
    "image file use edu program software graphic format jpeg data\n",
    "Topic 2:\n",
    "space nasa launch year satellite orbit system use earth mission\n",
    "\n",
    "We can name them as \"beliefs\" \"computer graphics\" and \"space science\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2.3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "atheism and religion are grouped together.\n",
    "This is because these two are both topic concerning people's faith and belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Find Dominant Topic in a Document\n",
    "\n",
    "Each document typically contains several topics. One of the topics is **dominant**, i.e. it is the largest topic in the document. That topic gives you an answer to the question: **What is this document about?** In other words, the document's dominant topic **summarizes** the document. \n",
    "\n",
    "Let's assign a dominant topic to **each document** in our corpus. Weights in a word vector for a topic provide a measure of association for the word with the topic. If you sum weights for a particular topic across all words in a document, you'll get the weight of that topic in the document.\n",
    "\n",
    "The attribute **.transform** to our function **lda_news** computes the weights of each topic in documents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_news_topic_weights = lda_news.transform(bow_news_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert lda_news_topic_weights into a nice-looking dataframe and have a look at the computed topic weights in documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc_0</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_1</th>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.3822</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.5959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_2</th>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.3423</td>\n",
       "      <td>0.6364</td>\n",
       "      <td>0.0106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_3</th>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.7682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_4</th>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.8737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_0  Topic_1  Topic_2  Topic_3\n",
       "Doc_0   0.0005   0.0005   0.9986   0.0004\n",
       "Doc_1   0.0193   0.3822   0.0026   0.5959\n",
       "Doc_2   0.0106   0.3423   0.6364   0.0106\n",
       "Doc_3   0.2103   0.0107   0.0108   0.7682\n",
       "Doc_4   0.0422   0.0424   0.0417   0.8737"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#array of document \"names\" and topic \"names\" (\"names\" are just indecies)\n",
    "doc_names = [\"Doc_\" + str(i) for i in range(len(normalized_corpus_news))]\n",
    "topic_names = [\"Topic_\" + str(i) for i in range(4)]\n",
    "\n",
    "#convert to dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_news_topic_weights, 4), columns=topic_names, index=doc_names)\n",
    "df_document_topic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in document Doc_0 the **dominant topic** is Topic_2 as it has the weight of 0.9986. The weights across the 4 topics sum up to 1. Let's add a column that shows dominant topic for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc_0</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_1</th>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.3822</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.5959</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_2</th>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.3423</td>\n",
       "      <td>0.6364</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_3</th>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.7682</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_4</th>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.8737</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_0  Topic_1  Topic_2  Topic_3  dominant_topic\n",
       "Doc_0   0.0005   0.0005   0.9986   0.0004               2\n",
       "Doc_1   0.0193   0.3822   0.0026   0.5959               3\n",
       "Doc_2   0.0106   0.3423   0.6364   0.0106               2\n",
       "Doc_3   0.2103   0.0107   0.0108   0.7682               3\n",
       "Doc_4   0.0422   0.0424   0.0417   0.8737               3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector of indecies for columns with the highest value by each row in df_document_topic\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "\n",
    "#add dominant_topic as a column to df_document_topic\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "df_document_topic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model Evaluation: Log-likelihood, Perplexity and Coherence Scores\n",
    "\n",
    "Log-likelihood, Perplexity and Coherence Score are **measures of performance** for a topic model. They are used for comparing and discriminating between topic models estimated on the same data. Log-likelihood, perplexity and coherence scores **do not have** a baseline or a threshold values and therefore are useful only for comparing models. \n",
    "\n",
    "How do you specify different models? You can set **different number of topics** and also play with the **parameters of the Dirichlet distributions**. \n",
    "\n",
    "#### Coherence Score\n",
    "\n",
    "We will use a function **CoherenceModel()** from the **gensim** module (you can also explore that package as it can be used to estimate an LDA model). The sklearn module does not have the functionality to compute the coherence score. Let's install the gensim package and the functions needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install gensim\n",
    "import gensim\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function CoherenceModel() needs as **inputs**:\n",
    "\n",
    "**1. Dictionary of the corpus**<br>\n",
    "**2. Corpus with each document represented as Bag-of-Words**<br>\n",
    "**3. An array of top words for each topic: we'll have top 20 words for each topic** \n",
    "  \n",
    "We will now create those objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the corpus\n",
    "news_corpus_tokenized = [tokenize_text(normalized_corpus_news[doc_id]) for doc_id in range(len(normalized_corpus_news))]\n",
    "\n",
    "#Dictionary of the corpus:\n",
    "news_dictionary = Dictionary(news_corpus_tokenized)\n",
    "\n",
    "#Bag-of-words representation for each document of the corpus:\n",
    "news_corpus_bow = [news_dictionary.doc2bow(doc) for doc in news_corpus_tokenized]\n",
    "\n",
    "#top 20 words for each topic (using the function defined in session prep)\n",
    "topic_topwords = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news, n_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute **the coherence score for the model overall**. We use one of the coherence metrics \"u-mass\" which measures semantic similarity of words in a topic, but there are other metrics as well.\n",
    "\n",
    "*Note: You can check out different coherence metrics here if you are interested: https://dl.acm.org/doi/abs/10.1145/2684822.2685324*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score for the model:  -1.4361\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_topwords, \n",
    "                    corpus = news_corpus_bow , \n",
    "                    dictionary = news_dictionary, coherence='u_mass')\n",
    "print(\"Coherence score for the model: \", np.round(cm.get_coherence(), 4))  # get coherence value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see **coherence scores by topic**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score by topic (higher values are better):  [-1.3688 -1.315  -1.3276 -1.7331]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence score by topic (higher values are better): \", np.round(cm.get_coherence_per_topic(),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Likelihood Score\n",
    "\n",
    "To compute the log-likelihood score we use the **.score** attribute of our defined and fitted LDA function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -741280.999399245\n"
     ]
    }
   ],
   "source": [
    "print(\"Log-Likelihood (higher values are better): \", lda_news.score(bow_news_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity Score\n",
    "\n",
    "To compute the Perplexity score we use the **.perplexity** attribute of our defined and fitted LDA function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (lower values are better):  574.401639146991\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (lower values are better): \", lda_news.perplexity(bow_news_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 3</font>**\n",
    "\n",
    "**<font color=green>Compare the coherence score, perplexity score and the log-likelihood for models with 2, 3, and 4 topics with your human-judgment-based evaluation of those models. What do you find? </font>**\n",
    "\n",
    "**<font color=green>What you need to do:</font>**\n",
    "\n",
    "**<font color=green>3.1. For model with 4 topics - All code work is done: The model and evaluation metrics are already computed above. You just need to look up the values for the coherence, perplexity and log-likelihood for the model with 4 topics above and discuss what you observe. You might be interested in looking at coherence score by topic as well;</font>**\n",
    "\n",
    "**<font color=green>3.2. For model with 3 topics - The model is computed in Exercise 2. You need to compute the perplexity, log-likelihood and coherence scores for the model with 3 topics (the lines for the coherence score are provided below) and dicuss your results;</font>**\n",
    "\n",
    "**<font color=green>3.3. For model with 2 topics - You need to fit the model with 2 topics and compute all 3 evaluation metrics; dicuss your results.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3.1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/0.25/0.25\n",
    "Log-Likelihood (higher values are better):  -740888.159696916\n",
    "Perplexity (lower values are better):  572.4709222471453\n",
    "Coherence score for the model: (higher values are better) -1.461\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -741177.8069456882\n",
      "Perplexity (lower values are better):  573.8938422547003\n",
      "Coherence score for the model: (higher values are better) -1.448\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "#Fit LDA with 4 topics:\n",
    "lda_news_test = LatentDirichletAllocation(n_components=4, max_iter=100,\n",
    "                                     doc_topic_prior = 0.3,\n",
    "                                     topic_word_prior = 0.3).fit(bow_news_corpus)\n",
    "\n",
    "\n",
    "#Log-Likelihood:\n",
    "print(\"Log-Likelihood (higher values are better): \", lda_news_test.score(bow_news_corpus))\n",
    "\n",
    "#Perplexity score:\n",
    "print(\"Perplexity (lower values are better): \", lda_news_test.perplexity(bow_news_corpus))\n",
    "\n",
    "\n",
    "#Coherence score for 3 topics:\n",
    "topic_topwords_test = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news_test, n_words=20)\n",
    "cm_2_test = CoherenceModel(topics=topic_topwords_test, \n",
    "                             corpus = news_corpus_bow, \n",
    "                             dictionary = news_dictionary, coherence='u_mass')\n",
    "\n",
    "#Overall coherence score for the model:\n",
    "print(\"Coherence score for the model: (higher values are better)\", np.round(cm_2_test.get_coherence(), 3))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3.2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code (complete the lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -743322.6031640773\n",
      "Perplexity (lower values are better):  584.5410049797958\n",
      "Coherence score for the model: (higher values are better) -1.442\n"
     ]
    }
   ],
   "source": [
    "#Log-Likelihood (add code):\n",
    "print(\"Log-Likelihood (higher values are better): \", lda_news_3_topics.score(bow_news_corpus))\n",
    "\n",
    "#Perplexity score (add code):\n",
    "print(\"Perplexity (lower values are better): \", lda_news_3_topics.perplexity(bow_news_corpus))\n",
    "\n",
    "#Coherence score for 3 topics:\n",
    "topic_topwords_3_topics = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news_3_topics, n_words=20)\n",
    "cm_3_topics = CoherenceModel(topics=topic_topwords_3_topics, \n",
    "                             corpus = news_corpus_bow, \n",
    "                             dictionary = news_dictionary, coherence='u_mass')\n",
    "\n",
    "#Overall coherence score for the model:\n",
    "print(\"Coherence score for the model: (higher values are better)\", np.round(cm_3_topics.get_coherence(), 3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with 3 topic seems to perform better than the model with 4 topics, because the Log-Likelihood, Perplexity and Coherence Score of 4-topic all outperforms those of the 3-topic one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3.3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "god people think know like jesus thing good believe even\n",
      "Topic 1:\n",
      "space image use file program system data edu nasa launch\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda_news_2_topics, bow_vectorizer_news.get_feature_names(), no_top_words_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -751830.7869127416\n",
      "Perplexity (lower values are better):  628.7592216537156\n",
      "Coherence score for the model: (higher values are better) -1.518\n"
     ]
    }
   ],
   "source": [
    "#Fit LDA with 2 topics:\n",
    "lda_news_2_topics = LatentDirichletAllocation(n_components=2, max_iter=100,\n",
    "                                     doc_topic_prior = 0.25,\n",
    "                                     topic_word_prior = 0.25).fit(bow_news_corpus)\n",
    "\n",
    "\n",
    "#Log-Likelihood:\n",
    "print(\"Log-Likelihood (higher values are better): \", lda_news_2_topics.score(bow_news_corpus))\n",
    "\n",
    "#Perplexity score:\n",
    "print(\"Perplexity (lower values are better): \", lda_news_2_topics.perplexity(bow_news_corpus))\n",
    "\n",
    "\n",
    "#Coherence score for 3 topics:\n",
    "topic_topwords_2_topics = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news_2_topics, n_words=20)\n",
    "cm_2_topics = CoherenceModel(topics=topic_topwords_2_topics, \n",
    "                             corpus = news_corpus_bow, \n",
    "                             dictionary = news_dictionary, coherence='u_mass')\n",
    "\n",
    "#Overall coherence score for the model:\n",
    "print(\"Coherence score for the model: (higher values are better)\", np.round(cm_2_topics.get_coherence(), 3))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our tests, the model with 3 topics performs generally better than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall discussion for EXERCISE 3**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**NOTE:** Generally, you can write a simple script that selects the best topic model **automatically** based on a criterion for \"best model\" (log-likelihood, perplexity, or coherence score). The script can vary both parameters of the Dirichlet distributions and the number of topics, or just the number of topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
