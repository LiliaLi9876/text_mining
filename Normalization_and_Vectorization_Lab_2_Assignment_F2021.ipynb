{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: TEXT NORMALIZATION and VECTORIZATION <br>\n",
    "\n",
    "\n",
    "**<font color=green>INSTRUCTIONS:</font>** <br> <br>\n",
    "    **<font color=green>1. Look for EXERCISES and QUESTIONS in this script. </font>** <br> <br>\n",
    "    **<font color=green>2. Each student INDIVIDUALLY uploads this script with their answers embedded (and other materials if requested) to Canvas by the the deadline indicated on Canvas.</font>** <br>\n",
    "## SESSION PREP\n",
    "\n",
    "### How to install any module from inside Jupyter\n",
    "\n",
    "To be able to install any module from inside Jupyper, we need module called sys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can install any module from Jupyter by running a line such as: <br> <br> !{sys.executable} -m pip install module_name\n",
    "\n",
    "### Install Natural Language ToolKit (NLTK) module (and some other modules)\n",
    "\n",
    "The NLTK module does text normalization, among other functions. We'll install module NLTK, as well as modules numpy and pandas, from inside Jupyter (you might see deprication warnings in pink about future changes in the module but you do not need to pay attention to them at this time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (1.20.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "import numpy as np \n",
    "\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download text data\n",
    "\n",
    "In what follows, we'll use an electronic archive of books from Project Gutenberg that Natural Language ToolKit has access to. In particular, we'll use \"Alice in Wonderland\" by Lewis Carrol. Our corpus will be just one file called carroll-alice.txt (it's in .txt format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/lilia/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[Alice's Adventures in Wonderland b\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg') \n",
    "from nltk.corpus import gutenberg \n",
    "\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt') # we name the corpus 'alice'\n",
    "from pprint import pprint #function for pretty printing\n",
    "pprint(alice[0:35]) #print the first 35 characters of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT TOKENIZATION\n",
    "**Tokenization** is splitting text into sematically meaningful chuncks, such as sentences or words. Tokenizing into words is most common. You might be interested in tokenizing into sentences if you plan to analyze text sentence by sentence.\n",
    "\n",
    "### Tokenization by Sentence\n",
    "From the NLTK module, we'll use a sentence tokenizer 'punkt':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now tokenize the Alice corpus by sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences in the corpus: 1625\n"
     ]
    }
   ],
   "source": [
    "alice_sentences = nltk.sent_tokenize(text=alice)\n",
    "print('\\nTotal sentences in the corpus:', len(alice_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first sentence in the Alice corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First sentence in alice: [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.\n"
     ]
    }
   ],
   "source": [
    "print('\\nFirst sentence in alice:', alice_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at what the second sentence looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second sentence in alice: Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n"
     ]
    }
   ],
   "source": [
    "print('\\nSecond sentence in alice:', alice_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>QUESTION 1: Why do you think the first and second tokenized sentences (above) look like that? (look at what Python printed out)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize at the level of sentences, divide the test into sentences with \".\" and \"?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization into Words\n",
    "Let's do some tokenization into words now. You can tokenize into words using punctuation signs, white spaces, or \"words\".\n",
    "\n",
    "We'll tokenize a corpus consisting of one sentence shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the races\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize **using \"words\"**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "print(words)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize **using punctuation signs** now. Do you see any difference between this tokenization and the previous one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize **using white spaces**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of stopwords (\"it's\", \"is\", \"the\", etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'into', 'couldn', 'theirs', 'than', 'those', 'below', \"aren't\", 'my', 'be', 'wouldn', 'having', 'all', \"couldn't\", 'did', 'after', \"mightn't\", 'but', 'itself', 'in', 'doing', 'won', 'what', 'm', 'them', \"wasn't\", 'with', \"needn't\", 'ma', \"it's\", 'how', 'didn', 'our', 'will', 'have', 'i', 'such', 'shouldn', 'haven', 'her', 'to', 'as', 'she', \"you'd\", 'once', 'over', 'until', \"should've\", 'been', 'ourselves', 'll', 'their', 'very', 've', 'mustn', 'yourself', 'from', 'has', \"you'll\", 'on', 'these', 'now', 'its', 're', 'do', 'up', 'any', 'each', \"doesn't\", 'yours', \"shan't\", 'same', 'under', 'before', \"that'll\", 'too', 'd', 'he', 't', 'only', 'more', 'doesn', 'other', 'themselves', 'at', 'where', 'off', 'does', 'by', 'for', 's', 'when', \"you're\", 'hadn', 'just', 'needn', 'through', 'again', 'further', 'a', 'yourselves', 'both', 'so', 'you', 'weren', 'being', 'ain', 'whom', 'that', 'me', 'then', \"haven't\", 'above', 'own', 'which', \"hadn't\", 'some', 'is', \"you've\", 'had', 'are', 'shan', 'while', 'o', 'should', 'we', 'why', 'aren', \"wouldn't\", 'not', 'here', 'nor', 'himself', 'hasn', 'mightn', 'there', 'or', 'wasn', 'this', 'if', \"don't\", 'don', 'your', 'isn', 'because', 'few', \"mustn't\", 'him', \"isn't\", 'myself', \"weren't\", 'between', 'hers', 'they', 'his', 'were', 'y', 'it', \"she's\", 'an', 'against', 'am', 'herself', 'about', 'who', 'was', 'of', 'most', \"hasn't\", 'out', 'can', 'no', \"shouldn't\", 'down', 'during', 'the', 'ours', \"won't\", \"didn't\", 'and'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can (and should consider) amending the list of stopwords given your data and project objectives. For example, we can add more stopwords to the standard list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'into', 'couldn', 'NYC', 'theirs', 'than', 'those', 'below', \"aren't\", 'my', 'be', 'wouldn', 'having', 'all', \"couldn't\", 'did', 'after', \"mightn't\", 'but', 'itself', 'in', 'doing', 'won', 'what', 'm', 'them', \"wasn't\", 'with', \"needn't\", 'ma', \"it's\", 'how', 'didn', 'our', 'will', 'have', 'i', 'such', 'shouldn', 'haven', 'her', 'to', 'as', 'she', \"you'd\", 'once', 'over', 'until', \"should've\", 'been', 'ourselves', 'll', 'their', 'very', 've', 'mustn', 'yourself', 'from', 'has', \"you'll\", 'on', 'these', 'now', 'its', 're', 'do', 'up', 'any', 'each', \"doesn't\", 'yours', \"shan't\", 'same', 'under', 'before', \"that'll\", 'too', 'd', 'he', 't', 'only', 'more', 'doesn', 'other', 'themselves', 'at', 'where', 'off', 'does', 'by', 'for', 's', 'when', \"you're\", 'hadn', 'just', 'needn', 'through', 'again', 'further', 'a', 'yourselves', 'both', 'so', 'you', 'weren', 'being', 'ain', 'whom', 'that', 'me', 'then', \"haven't\", 'above', 'own', 'which', \"hadn't\", 'some', 'is', \"you've\", 'had', 'are', 'shan', 'while', 'o', 'should', 'we', 'why', 'aren', \"wouldn't\", 'not', 'here', 'nor', 'himself', 'hasn', 'mightn', 'there', 'or', 'wasn', 'this', 'if', \"don't\", 'don', 'your', 'isn', 'because', 'few', \"mustn't\", 'him', \"isn't\", 'myself', \"weren't\", 'between', 'hers', 'they', 'his', 'were', 'y', 'it', \"she's\", 'an', 'against', 'am', 'herself', 'about', 'who', 'was', 'of', 'most', \"hasn't\", 'out', 'can', 'no', \"shouldn't\", 'down', 'during', 'the', 'ours', \"won't\", \"didn't\", 'and'}\n"
     ]
    }
   ],
   "source": [
    "add_stopwords ={'so','NYC'}\n",
    "stop_words_new = add_stopwords.union(stop_words)\n",
    "print(stop_words_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the tokenized sentence before and after removing the stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n",
      "Filterd Sentence (without stopwords): ['The', 'brown', 'fox', 'quick', 'win', 'races']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens=[]\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "        \n",
    "print(\"Tokenized Sentence:\",words)\n",
    "print(\"Filterd Sentence (without stopwords):\",filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING AND LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stem the sentence first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['The', 'brown', 'fox', 'quick', 'win', 'races']\n",
      "Stemmed Sentence: ['the', 'brown', 'fox', 'quick', 'win', 'race']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_tokens=[]\n",
    "for w in filtered_tokens:\n",
    "    stemmed_tokens.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_tokens)\n",
    "print(\"Stemmed Sentence:\",stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare stemming to lemmatization for the word \"running\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: run\n",
      "Stemmed Word: run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "word = \"running\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more comparison for the word \"bought\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: buy\n",
      "Stemmed Word: bought\n"
     ]
    }
   ],
   "source": [
    "word = \"bought\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb (part-of-speech)\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>EXERCISE 1: What result would you get if you change the part-of-speech tag in the lemmatization line above to \"n\", which means \"noun\"? (look at what Python printed out)</font> <br>\n",
    "Your Answer in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: bought\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"n\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text vectorization is the process of feature extraction from text data, that is the process of creating variables for each observation, where an observation is a text document. We'll consider the **bag-of-words**, the **TF-IDF** and the **n-grams** vectorized representations of text. <br>\n",
    "\n",
    "Let's vectorize the corpus about \"blue skies and blue cheese\" similar to one used in the video lecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['the sky is blue',\n",
    "          'sky is blue and sky is beautiful', \n",
    "          'the beautiful sky is so blue',\n",
    "          'i love blue cheese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use built-in vectorizers from Scikit-Learn module for machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words Representation\n",
    "\n",
    "We'll use bag-of-words representation (CountVectorizer) first. You can see the documentation here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convinient to \"define\" a vectorizer first before applying it. You can specify all the parameters (arguments) of the function in the definition. For example, the max_features parameter below drops all features except for the selected number of most frequent terms in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_BOW = CountVectorizer(max_features=1000) #BOW = bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract features using the vectorizer function. Note the .fit_transform function below. It creates the dictionary of the corpus and does the vectorization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8\n",
       "0  0  0  1  0  1  0  1  0  1\n",
       "1  1  1  1  0  2  0  2  0  0\n",
       "2  0  1  1  0  1  0  1  1  1\n",
       "3  0  0  1  1  0  1  0  0  0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_matrix = vectorizer_BOW.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(np.round(BOW_matrix,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to attach the names of the features, right? Here are the names of the features from the dictionary of the corpus (note the function get_feature_names()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_BOW.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a more useful looking bag-of-words representation, with feature names attached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
       "0    0          0     1       0   1     0    1   0    1\n",
       "1    1          1     1       0   2     0    2   0    0\n",
       "2    0          1     1       0   1     0    1   1    1\n",
       "3    0          0     1       1   0     1    0   0    0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(BOW_matrix,2),columns=vectorizer_BOW.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Using N-grams\n",
    "<br>\n",
    "Let's use bi-grams in our vectorized representation of text. First, we define the vectorizer (we need the same CountVectorizer() function) using a parameter for specifying n-grams. Then we apply it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and sky</th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>blue and</th>\n",
       "      <th>blue cheese</th>\n",
       "      <th>is beautiful</th>\n",
       "      <th>is blue</th>\n",
       "      <th>is so</th>\n",
       "      <th>love blue</th>\n",
       "      <th>sky is</th>\n",
       "      <th>so blue</th>\n",
       "      <th>the beautiful</th>\n",
       "      <th>the sky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and sky  beautiful sky  blue and  blue cheese  is beautiful  is blue  \\\n",
       "0        0              0         0            0             0        1   \n",
       "1        1              0         1            0             1        1   \n",
       "2        0              1         0            0             0        0   \n",
       "3        0              0         0            1             0        0   \n",
       "\n",
       "   is so  love blue  sky is  so blue  the beautiful  the sky  \n",
       "0      0          0       1        0              0        1  \n",
       "1      0          0       2        0              0        0  \n",
       "2      1          0       1        1              1        0  \n",
       "3      0          1       0        0              0        0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_Bi_Grams = CountVectorizer(max_features=1000, ngram_range=(2, 2))\n",
    "Bi_Grams_matrix = vectorizer_Bi_Grams.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(np.round(Bi_Grams_matrix,2),columns=vectorizer_Bi_Grams.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>EXERCISE 2: Create a Bi-Grams vectorizer that uses the mix of bi-grams and uni-grams. To complete the Exercise you may need to look up CountVectorizer's documentation, see link below.</font> <br>\n",
    "\n",
    "Documentation: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html <br>\n",
    "\n",
    "Your Answer in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>and sky</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>blue</th>\n",
       "      <th>blue and</th>\n",
       "      <th>blue cheese</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>is beautiful</th>\n",
       "      <th>...</th>\n",
       "      <th>is so</th>\n",
       "      <th>love</th>\n",
       "      <th>love blue</th>\n",
       "      <th>sky</th>\n",
       "      <th>sky is</th>\n",
       "      <th>so</th>\n",
       "      <th>so blue</th>\n",
       "      <th>the</th>\n",
       "      <th>the beautiful</th>\n",
       "      <th>the sky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  and sky  beautiful  beautiful sky  blue  blue and  blue cheese  \\\n",
       "0    0        0          0              0     1         0            0   \n",
       "1    1        1          1              0     1         1            0   \n",
       "2    0        0          1              1     1         0            0   \n",
       "3    0        0          0              0     1         0            1   \n",
       "\n",
       "   cheese  is  is beautiful  ...  is so  love  love blue  sky  sky is  so  \\\n",
       "0       0   1             0  ...      0     0          0    1       1   0   \n",
       "1       0   2             1  ...      0     0          0    2       2   0   \n",
       "2       0   1             0  ...      1     0          0    1       1   1   \n",
       "3       1   0             0  ...      0     1          1    0       0   0   \n",
       "\n",
       "   so blue  the  the beautiful  the sky  \n",
       "0        0    1              0        1  \n",
       "1        0    0              0        0  \n",
       "2        1    1              1        0  \n",
       "3        0    0              0        0  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_Uni_Bi_Grams = CountVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "Uni_Bi_Grams_matrix = vectorizer_Uni_Bi_Grams.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(np.round(Uni_Bi_Grams_matrix,2),columns=vectorizer_Uni_Bi_Grams.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with Term Frequency – Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Now, let's do feature extraction (vectorization) using the TF-IDF approach. <br> <br> See full documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer <br> <br>\n",
    "Import the vectorizer first and define it by specifying the functions (look up the specified parameters in the documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer_TF_IDF = TfidfVectorizer(norm = None, smooth_idf = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize our corpus now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.92</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
       "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
       "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
       "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
       "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(np.round(TF_IDF_matrix, 2), columns=vectorizer_TF_IDF.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the IDF weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.92 1.51 1.   1.92 1.22 1.92 1.22 1.92 1.51]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(vectorizer_TF_IDF.idf_,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to normalize the TF-IDF matrix, i.e. restrict all entries to be between 0 and 1. Some text mining models require normalized matrices. Norm parameter is used for this purpose (you can look it up in the documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
       "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
       "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
       "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
       "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_TF_IDF = TfidfVectorizer(norm = 'l2', smooth_idf = True)\n",
    "TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).todense()\n",
    "pd.DataFrame(np.round(TF_IDF_matrix,2), columns=vectorizer_TF_IDF.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green> EXERCISE 3: You are given a new small corpus called corpus_exercise (see below). Your ultimate task is to normalize (pre-process) the corpus and produce the TF-IDF and the Bag-of-Words representations of the data. Follow the steps below to complete this exercise:</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Download a file Text_Normalization_Function.ipynb from Canvas and put it into the same directory(!) as the current Jupyter notebook. That file defines a relatively sophisticated text normalization function. (OPTIONAL: you can explore what that file does when you are done with this exercise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Run the file Text_Normalization_Function.ipynb to define the text normalization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html.parser\n",
      "  Downloading html-parser-0.2.tar.gz (904 bytes)\n",
      "Requirement already satisfied: ply in /opt/anaconda3/lib/python3.8/site-packages (from html.parser) (3.11)\n",
      "Building wheels for collected packages: html.parser\n",
      "  Building wheel for html.parser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for html.parser: filename=html_parser-0.2-py3-none-any.whl size=1333 sha256=bb3688801ac24edb20728b7c25472a71c3ce7cfd5776cf8c0a5834e9c0d51b1e\n",
      "  Stored in directory: /Users/lilia/Library/Caches/pip/wheels/4f/85/2a/67a30aa6cf144eca0c159f337ce5166df2213c4cde9e699cbe\n",
      "Successfully built html.parser\n",
      "Installing collected packages: html.parser\n",
      "Successfully installed html.parser\n",
      "Collecting pattern3\n",
      "  Downloading pattern3-3.0.0.tar.gz (23.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7 MB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (4.9.3)\n",
      "Collecting cherrypy\n",
      "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
      "\u001b[K     |████████████████████████████████| 419 kB 24.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docx\n",
      "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 5.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting feedparser\n",
      "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pdfminer3k\n",
      "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 14.0 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting simplejson\n",
      "  Downloading simplejson-3.17.5-cp38-cp38-macosx_10_9_x86_64.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pdfminer.six\n",
      "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->pattern3) (2.2.1)\n",
      "Collecting cheroot>=8.2.1\n",
      "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jaraco.collections\n",
      "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (8.7.0)\n",
      "Collecting zc.lockfile\n",
      "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting portend>=2.1.1\n",
      "  Downloading portend-3.0.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (1.15.0)\n",
      "Collecting jaraco.functools\n",
      "  Downloading jaraco.functools-3.4.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting tempora>=1.8\n",
      "  Downloading tempora-4.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.8/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2021.1)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.8/site-packages (from docx->pattern3) (4.6.3)\n",
      "Requirement already satisfied: Pillow>=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from docx->pattern3) (8.2.0)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Collecting jaraco.classes\n",
      "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting jaraco.text\n",
      "  Downloading jaraco.text-3.6.0-py3-none-any.whl (8.1 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern3) (3.4.1)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer.six->pattern3) (4.0.0)\n",
      "Requirement already satisfied: cryptography in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer.six->pattern3) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.8/site-packages (from cryptography->pdfminer.six->pattern3) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern3) (2.20)\n",
      "Requirement already satisfied: ply in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from zc.lockfile->cherrypy->pattern3) (52.0.0.post20210125)\n",
      "Building wheels for collected packages: pattern3, docx, sgmllib3k\n",
      "  Building wheel for pattern3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pattern3: filename=pattern3-3.0.0-py2.py3-none-any.whl size=18554332 sha256=e1e8c402c77a3a740c5d5339877834854a9623f98d50c869e1c9f81a979a456a\n",
      "  Stored in directory: /Users/lilia/Library/Caches/pip/wheels/15/b7/69/d81044899221ca012f556e2059ce7c58cb90fbd0fd9a395c4d\n",
      "  Building wheel for docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53925 sha256=e60fe5c28cbb06ec8111a833b9933436749b7a90ae7a2aba608e9ba59769b68c\n",
      "  Stored in directory: /Users/lilia/Library/Caches/pip/wheels/78/ae/2e/5ce789557ca59281a463dabe9d6522e429be240322148e5379\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=b29293144603cf9db947d77cc4a8578493b2922dd644bc447bc24be00955d18b\n",
      "  Stored in directory: /Users/lilia/Library/Caches/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built pattern3 docx sgmllib3k\n",
      "Installing collected packages: jaraco.functools, importlib-resources, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cheroot, simplejson, pdfminer3k, pdfminer.six, feedparser, docx, cherrypy, pattern3\n",
      "Successfully installed cheroot-8.5.2 cherrypy-18.6.1 docx-0.2.4 feedparser-6.0.8 importlib-resources-5.4.0 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.4.0 jaraco.text-3.6.0 pattern3-3.0.0 pdfminer.six-20211012 pdfminer3k-1.3.4 portend-3.0.0 sgmllib3k-1.0.0 simplejson-3.17.5 tempora-4.1.2 zc.lockfile-2.0\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.2.4)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 630 kB/s eta 0:00:011    |█████████                       | 6.7 MB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.24.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.6.2)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.11.3)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 18.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn\n",
      "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=e77b2b2b44890ccd5f54d96151f0fca81ca81ded67f0bebc55ad9c4323add697\n",
      "  Stored in directory: /Users/lilia/Library/Caches/pip/wheels/90/61/ec/9dbe9efc3acf9c4e37ba70fbbcc3f3a0ebd121060aa593181a\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=8cfb9de4a60e3abc20de71a6b5b516b7ea7f52527027681da14d5e5fff9406d6\n",
      "  Stored in directory: /Users/lilia/Library/Caches/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built pyLDAvis sklearn\n",
      "Installing collected packages: smart-open, sklearn, gensim, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.16 gensim-4.1.2 pyLDAvis-3.3.1 sklearn-0.0 smart-open-5.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lilia/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/lilia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run ./Text_Normalization_Function.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Define the corpus_exercise text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "corpus_exercise = ['python is great for text mining',\n",
    "          'anyone can learn python and do text mining', \n",
    "          'python can go without eating for days',\n",
    "          'python can be a great pet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Normalize the corpus_exercise text corpus and call its normalized version NORM_corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['python great text mining',\n",
       " 'anyone learn python text mining',\n",
       " 'python without eat day',\n",
       " 'python great pet']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_corpus = normalize_corpus(corpus_exercise)\n",
    "NORM_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Compute and print out the TF-IDF and the Bag-of-Words representations for NORM_corpus (WRITE the lines of code needed in the cell below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anyone</th>\n",
       "      <th>day</th>\n",
       "      <th>eat</th>\n",
       "      <th>great</th>\n",
       "      <th>learn</th>\n",
       "      <th>mining</th>\n",
       "      <th>pet</th>\n",
       "      <th>python</th>\n",
       "      <th>text</th>\n",
       "      <th>without</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   anyone   day   eat  great  learn  mining   pet  python  text  without\n",
       "0    0.00  0.00  0.00   0.54   0.00    0.54  0.00    0.36  0.54     0.00\n",
       "1    0.53  0.00  0.00   0.00   0.53    0.42  0.00    0.28  0.42     0.00\n",
       "2    0.00  0.55  0.55   0.00   0.00    0.00  0.00    0.29  0.00     0.55\n",
       "3    0.00  0.00  0.00   0.57   0.00    0.00  0.73    0.38  0.00     0.00"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_vectorizer_TF_IDF = TfidfVectorizer(norm = 'l2', smooth_idf = True)\n",
    "#l2: 0-1\n",
    "NORM_TF_IDF_matrix = NORM_vectorizer_TF_IDF.fit_transform(NORM_corpus).todense()\n",
    "pd.DataFrame(np.round(NORM_TF_IDF_matrix, 2), columns=NORM_vectorizer_TF_IDF.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anyone</th>\n",
       "      <th>day</th>\n",
       "      <th>eat</th>\n",
       "      <th>great</th>\n",
       "      <th>learn</th>\n",
       "      <th>mining</th>\n",
       "      <th>pet</th>\n",
       "      <th>python</th>\n",
       "      <th>text</th>\n",
       "      <th>without</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   anyone  day  eat  great  learn  mining  pet  python  text  without\n",
       "0       0    0    0      1      0       1    0       1     1        0\n",
       "1       1    0    0      0      1       1    0       1     1        0\n",
       "2       0    1    1      0      0       0    0       1     0        1\n",
       "3       0    0    0      1      0       0    1       1     0        0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_BOW_matrix = vectorizer_BOW.fit_transform(NORM_corpus).toarray()\n",
    "\n",
    "pd.DataFrame(np.round(NORM_BOW_matrix,2),columns=vectorizer_BOW.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green> OPTIONAL EXERCISE 4: Explore the Text_Normalization_Function.ipynb notebook that defines a text normalization function. The file is available on Canvas in the Lab 2 Assignment (no answer is needed). </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
